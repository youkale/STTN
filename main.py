# -*- coding: utf-8 -*-
import cv2
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
import math
import time
import importlib
import os
import argparse
import copy
import datetime
import random
import sys
import json
import subprocess

import torch
from torch.autograd import Variable

import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init
import torch.utils.model_zoo as model_zoo
from torchvision import models
import torch.multiprocessing as mp
from torchvision import transforms

# My libs
from src.core.utils import Stack, ToTorchFormatTensor


parser = argparse.ArgumentParser(description="STTN")
parser.add_argument("-v", "--video", type=str, required=True, help="è¾“å…¥è§†é¢‘è·¯å¾„")
parser.add_argument("-m", "--mask",   type=str, help="maskå›¾ç‰‡ç›®å½•è·¯å¾„ï¼ˆä¸--regionsäºŒé€‰ä¸€ï¼‰")
parser.add_argument("-c", "--ckpt",   type=str, required=True, help="æ¨¡å‹checkpointè·¯å¾„")
parser.add_argument("--model",   type=str, default='sttn', help="æ¨¡å‹åç§°")
parser.add_argument("--regions", type=str, help="éœ€è¦æ“¦é™¤çš„åŒºåŸŸåæ ‡ï¼Œæ ¼å¼ï¼š[[left,bottom,right,top],...]ï¼Œåæ ‡ä¸ºç›¸å¯¹æ¯”ä¾‹(0-1)ï¼ŒåŸç‚¹åœ¨å·¦ä¸‹è§’")
parser.add_argument("--output", type=str, help="è¾“å‡ºè§†é¢‘è·¯å¾„ï¼ˆé»˜è®¤ï¼š{mask}_result.mp4ï¼‰")
parser.add_argument("--resolution", type=str, help="å¤„ç†åˆ†è¾¨ç‡ï¼Œæ ¼å¼ï¼šWxH (å¦‚ 1280x720)ï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹")
parser.add_argument("--scale", type=float, default=1.0, help="åˆ†è¾¨ç‡ç¼©æ”¾æ¯”ä¾‹ (0-1]ï¼Œé»˜è®¤1.0ä¿æŒåŸåˆ†è¾¨ç‡")
parser.add_argument("--short-side", type=int, choices=[270, 360, 480, 540, 720, 1080],
                    help="ç­‰æ¯”ç¼©æ”¾åˆ°æŒ‡å®šçŸ­è¾¹å°ºå¯¸ï¼Œä¿æŒå®½é«˜æ¯” (270/360/480/540/720/1080)")
parser.add_argument("--crop", action="store_true",
                    help="è£å‰ªæ¨¡å¼ï¼šä»…å¤„ç†åŒºåŸŸå‘¨å›´éƒ¨åˆ†ï¼Œå¤§å¹…é™ä½æ˜¾å­˜å ç”¨ï¼ˆéœ€é…åˆ--regionsä½¿ç”¨ï¼‰")
parser.add_argument("--crop-padding", type=int, default=32,
                    help="è£å‰ªæ—¶çš„è¾¹ç•Œpaddingï¼ˆåƒç´ ï¼‰ï¼Œé¿å…è¾¹ç•Œartifactsï¼Œé»˜è®¤32")
parser.add_argument("--auto-split", action="store_true",
                    help="è‡ªåŠ¨åˆ†æ®µå¤„ç†ï¼šæ ¹æ®æ˜¾å­˜å¤§å°è‡ªåŠ¨åˆ‡åˆ†è§†é¢‘ï¼Œé€æ®µå¤„ç†ååˆå¹¶")
parser.add_argument("--max-frames", type=int,
                    help="æ¯æ®µæœ€å¤§å¸§æ•°ï¼ˆé…åˆ--auto-splitä½¿ç”¨ï¼‰ï¼Œä¸æŒ‡å®šåˆ™è‡ªåŠ¨è®¡ç®—")
args = parser.parse_args()


# é»˜è®¤å€¼ï¼Œå°†åœ¨ get_video_info ä¸­æ›´æ–°
w, h = 432, 240
ref_length = 10
neighbor_stride = 5
default_fps = 24

_to_tensors = transforms.Compose([
    Stack(),
    ToTorchFormatTensor()])


def get_video_info_ffprobe(video_path):
    """
    ä½¿ç”¨ ffprobe è·å–è§†é¢‘ä¿¡æ¯

    è¿”å›: (width, height, fps) æˆ– None
    """
    try:
        cmd = [
            'ffprobe',
            '-v', 'error',
            '-select_streams', 'v:0',
            '-show_entries', 'stream=width,height,r_frame_rate',
            '-of', 'json',
            video_path
        ]

        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)

        if result.returncode == 0:
            data = json.loads(result.stdout)
            if 'streams' in data and len(data['streams']) > 0:
                stream = data['streams'][0]
                width = stream.get('width')
                height = stream.get('height')

                # è§£æå¸§ç‡ (æ ¼å¼å¯èƒ½æ˜¯ "30/1" æˆ– "30000/1001")
                fps_str = stream.get('r_frame_rate', '24/1')
                if '/' in fps_str:
                    num, den = map(int, fps_str.split('/'))
                    fps = num / den if den != 0 else 24
                else:
                    fps = float(fps_str)

                return width, height, fps
    except (subprocess.TimeoutExpired, FileNotFoundError, json.JSONDecodeError) as e:
        print(f"âš ï¸  ffprobe è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}")

    return None


def get_video_info_opencv(video_path):
    """
    ä½¿ç”¨ OpenCV è·å–è§†é¢‘ä¿¡æ¯ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰

    è¿”å›: (width, height, fps)
    """
    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")

    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)

    cap.release()

    # å¦‚æœ fps æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼
    if fps <= 0 or fps > 120:
        fps = 24

    return width, height, fps


def setup_resolution(video_path, resolution_arg=None, scale=1.0, short_side=None):
    """
    è®¾ç½®å¤„ç†åˆ†è¾¨ç‡

    å‚æ•°:
        video_path: è§†é¢‘è·¯å¾„
        resolution_arg: ç”¨æˆ·æŒ‡å®šçš„åˆ†è¾¨ç‡å­—ç¬¦ä¸² (å¦‚ "1280x720")
        scale: ç¼©æ”¾æ¯”ä¾‹
        short_side: çŸ­è¾¹ç›®æ ‡å°ºå¯¸ï¼ˆç­‰æ¯”ç¼©æ”¾ï¼‰

    è¿”å›: (width, height, fps)
    """
    global w, h, default_fps

    # 1. å¦‚æœç”¨æˆ·æŒ‡å®šäº†åˆ†è¾¨ç‡ï¼Œç›´æ¥ä½¿ç”¨
    if resolution_arg:
        try:
            w, h = map(int, resolution_arg.lower().split('x'))
            print(f"âœ“ ä½¿ç”¨æŒ‡å®šåˆ†è¾¨ç‡: {w}x{h}")
        except ValueError:
            print(f"âš ï¸  æ— æ•ˆçš„åˆ†è¾¨ç‡æ ¼å¼: {resolution_arg}ï¼Œå°†è‡ªåŠ¨æ£€æµ‹")
            resolution_arg = None

    # 2. è·å–è§†é¢‘ä¿¡æ¯
    if not resolution_arg:
        # ä¼˜å…ˆä½¿ç”¨ ffprobe
        video_info = get_video_info_ffprobe(video_path)

        # å¦‚æœ ffprobe å¤±è´¥ï¼Œä½¿ç”¨ OpenCV
        if video_info is None:
            print("âš ï¸  ffprobe ä¸å¯ç”¨ï¼Œä½¿ç”¨ OpenCV è·å–è§†é¢‘ä¿¡æ¯")
            video_info = get_video_info_opencv(video_path)

        orig_w, orig_h, fps = video_info

        # åº”ç”¨çŸ­è¾¹ç¼©æ”¾ï¼ˆä¼˜å…ˆçº§é«˜äº scaleï¼‰
        if short_side:
            current_short = min(orig_w, orig_h)
            current_long = max(orig_w, orig_h)

            # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
            scale_ratio = short_side / current_short

            # åº”ç”¨ç¼©æ”¾
            if orig_w < orig_h:  # ç«–å±
                w = short_side
                h = int(current_long * scale_ratio)
            else:  # æ¨ªå±
                h = short_side
                w = int(current_long * scale_ratio)

            print(f"âœ“ åŸå§‹åˆ†è¾¨ç‡: {orig_w}x{orig_h}")
            print(f"âœ“ ç­‰æ¯”ç¼©æ”¾åˆ°çŸ­è¾¹ {short_side}: {w}x{h}")
        # åº”ç”¨ç¼©æ”¾æ¯”ä¾‹
        elif scale != 1.0:
            if scale <= 0 or scale > 1:
                print(f"âš ï¸  æ— æ•ˆçš„ç¼©æ”¾æ¯”ä¾‹ {scale}ï¼Œä½¿ç”¨ 1.0")
                scale = 1.0
            w = int(orig_w * scale)
            h = int(orig_h * scale)
            print(f"âœ“ åŸå§‹åˆ†è¾¨ç‡: {orig_w}x{orig_h}")
            print(f"âœ“ ç¼©æ”¾æ¯”ä¾‹: {scale}")
            print(f"âœ“ å¤„ç†åˆ†è¾¨ç‡: {w}x{h}")
        else:
            w = orig_w
            h = orig_h
            print(f"âœ“ è‡ªåŠ¨æ£€æµ‹åˆ†è¾¨ç‡: {w}x{h}")

        # æ›´æ–°å¸§ç‡
        default_fps = int(fps)
        print(f"âœ“ æ£€æµ‹åˆ°å¸§ç‡: {default_fps} fps")

    # 3. ç¡®ä¿åˆ†è¾¨ç‡æ˜¯å¶æ•°ï¼ˆè§†é¢‘ç¼–ç è¦æ±‚ï¼‰
    w = w if w % 2 == 0 else w + 1
    h = h if h % 2 == 0 else h + 1

    # 4. ç¡®ä¿åˆ†è¾¨ç‡åŒ¹é…æ¨¡å‹patchsizeè¦æ±‚
    # æ¨¡å‹patchsizeç¡¬ç¼–ç ä¸º [(108, 60), ...], é’ˆå¯¹432x240è®¾è®¡
    # å¤„ç†åˆ†è¾¨ç‡å¿…é¡»æ˜¯432x240çš„å€æ•°
    base_w, base_h = 432, 240

    # è®¡ç®—æœ€æ¥è¿‘çš„åˆæ³•åˆ†è¾¨ç‡
    w_ratio = w / base_w
    h_ratio = h / base_h

    # å‘ä¸‹å–æ•´åˆ°æœ€æ¥è¿‘çš„å€æ•°ï¼ˆä¿å®ˆç­–ç•¥ï¼Œé¿å…æ˜¾å­˜è¶…æ ‡ï¼‰
    w_mult = max(1, int(w_ratio))
    h_mult = max(1, int(h_ratio))

    # å¦‚æœåŸå§‹åˆ†è¾¨ç‡ä¸åŒ¹é…ï¼Œè°ƒæ•´å®ƒ
    adjusted_w = base_w * w_mult
    adjusted_h = base_h * h_mult

    if adjusted_w != w or adjusted_h != h:
        print(f"\nâš ï¸  åˆ†è¾¨ç‡è°ƒæ•´:")
        print(f"   åŸåˆ†è¾¨ç‡: {w}x{h}")
        print(f"   è°ƒæ•´å: {adjusted_w}x{adjusted_h}")
        print(f"   åŸå› : æ¨¡å‹è¦æ±‚åˆ†è¾¨ç‡å¿…é¡»æ˜¯ {base_w}x{base_h} çš„å€æ•°")
        w, h = adjusted_w, adjusted_h

    # 5. æ˜¾ç¤ºæ˜¾å­˜ä¼°ç®—ï¼ˆè¿™é‡Œè¿˜ä¸çŸ¥é“è§†é¢‘å¸§æ•°ï¼‰
    estimate_memory(w, h)

    return w, h, default_fps


def estimate_memory(width, height, video_length=None):
    """
    ä¼°ç®—æ‰€éœ€æ˜¾å­˜

    å‚æ•°:
        width, height: è§†é¢‘åˆ†è¾¨ç‡
        video_length: è§†é¢‘å¸§æ•°ï¼ˆç”¨äºå‡†ç¡®ä¼°ç®—ï¼‰
    """
    pixels = width * height

    # åŸºç¡€ä¼°ç®—ï¼ˆå‡è®¾100å¸§ï¼‰
    base_frames = 100
    estimated_gb_per_100frames = (pixels / 1_000_000) * 6

    print(f"ğŸ“Š åˆ†è¾¨ç‡ä¿¡æ¯:")
    print(f"   åƒç´ æ•°: {pixels:,} ({pixels/1_000_000:.2f}M)")

    if video_length:
        # ç²¾ç¡®ä¼°ç®—ï¼šè€ƒè™‘å®é™…å¸§æ•°
        # æ˜¾å­˜å ç”¨ä¸»è¦æ¥è‡ªï¼šfeats + masks + ä¸­é—´tensor
        # feats: (1, T, C, H/4, W/4) float32 = T * 256 * (H/4) * (W/4) * 4 bytes
        # masks: (1, T, 1, H, W) float32 = T * H * W * 4 bytes
        feat_h, feat_w = height // 4, width // 4
        feats_gb = (video_length * 256 * feat_h * feat_w * 4) / (1024**3)
        masks_gb = (video_length * height * width * 4) / (1024**3)
        frames_gb = (video_length * height * width * 3) / (1024**3)  # RGB
        overhead_gb = 1.5  # æ¨¡å‹æƒé‡ + ä¸­é—´å˜é‡

        estimated_gb = feats_gb + masks_gb + frames_gb + overhead_gb

        print(f"   è§†é¢‘å¸§æ•°: {video_length}")
        print(f"   è§†é¢‘æ—¶é•¿: ~{video_length/24:.1f}ç§’ (å‡è®¾24fps)")
        print(f"   æ˜¾å­˜å ç”¨ä¼°ç®—:")
        print(f"     - ç‰¹å¾å¼ é‡: {feats_gb:.2f}GB")
        print(f"     - Maskå¼ é‡: {masks_gb:.2f}GB")
        print(f"     - å¸§æ•°æ®: {frames_gb:.2f}GB")
        print(f"     - æ¨¡å‹+å¼€é”€: {overhead_gb:.2f}GB")
        print(f"   æ€»è®¡: ~{estimated_gb:.1f}GB")
    else:
        # ç²—ç•¥ä¼°ç®—ï¼ˆå‡è®¾100å¸§ï¼‰
        estimated_gb = estimated_gb_per_100frames
        print(f"   ä¼°ç®—æ˜¾å­˜ (100å¸§): ~{estimated_gb:.1f}GB")
        print(f"   âš ï¸  å®é™…æ˜¾å­˜å–å†³äºè§†é¢‘é•¿åº¦")

    # æ£€æµ‹å®é™…å¯ç”¨æ˜¾å­˜
    if torch.cuda.is_available():
        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"   GPU æ€»æ˜¾å­˜: {gpu_memory_gb:.1f}GB")

        if video_length:
            # æ˜¾ç¤ºå½“å‰GPUçŠ¶æ€
            try:
                torch.cuda.empty_cache()
                allocated = torch.cuda.memory_allocated(0) / (1024**3)
                reserved = torch.cuda.memory_reserved(0) / (1024**3)
                free = gpu_memory_gb - reserved
                print(f"   å½“å‰çŠ¶æ€: å·²åˆ†é… {allocated:.2f}GB, ä¿ç•™ {reserved:.2f}GB, å¯ç”¨ {free:.2f}GB")
            except:
                pass

        if video_length and estimated_gb > gpu_memory_gb * 0.85:
            print(f"\n   âŒ é”™è¯¯: ä¼°ç®—æ˜¾å­˜ ({estimated_gb:.1f}GB) è¶…è¿‡ GPU å®¹é‡ ({gpu_memory_gb:.1f}GB)")

            # è®¡ç®—å»ºè®®çš„æœ€å¤§å¸§æ•°
            max_frames = int((gpu_memory_gb * 0.8 - 1.5) / ((256 * (height//4) * (width//4) * 4 + height * width * 4 + height * width * 3) / (1024**3)))
            max_seconds = max_frames / 24

            print(f"\n   ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
            print(f"   1. è£å‰ªè§†é¢‘æ—¶é•¿: æœ€å¤š {max_frames} å¸§ (~{max_seconds:.1f}ç§’)")
            print(f"      ffmpeg -i input.mp4 -t {int(max_seconds)} output.mp4")
            print(f"\n   2. é™ä½åˆ†è¾¨ç‡:")
            print(f"      --short-side 360  æˆ–  --short-side 480")
            print(f"\n   3. ä½¿ç”¨è£å‰ªæ¨¡å¼ (æ¨è):")
            print(f"      --crop  (æ˜¾å­˜å¯é™ä½10-40å€)")
            print()
            import sys
            sys.exit(1)
        elif not video_length and estimated_gb > gpu_memory_gb * 0.8:
            print(f"\n   âš ï¸  è­¦å‘Š: åŸºç¡€ä¼°ç®—æ˜¾å­˜å¯èƒ½ä¸è¶³")
            print(f"   å»ºè®®ä½¿ç”¨ --crop æˆ–é™ä½åˆ†è¾¨ç‡")

    return estimated_gb


# sample reference frames from the whole video
def get_ref_index(neighbor_ids, length):
    ref_index = []
    for i in range(0, length, ref_length):
        if not i in neighbor_ids:
            ref_index.append(i)
    return ref_index


# read frame-wise masks
def read_mask(mpath, video_length=None):
    """
    è¯»å–maskå›¾ç‰‡

    å‚æ•°:
        mpath: maskç›®å½•è·¯å¾„
        video_length: è§†é¢‘æ€»å¸§æ•°ï¼Œå¦‚æœæä¾›ä¸”ç›®å½•åªæœ‰ä¸€å¼ å›¾ç‰‡ï¼Œåˆ™å¤ç”¨è¯¥å›¾ç‰‡

    è¿”å›:
        masks: maskå›¾ç‰‡åˆ—è¡¨
    """
    masks = []
    mnames = [f for f in os.listdir(mpath) if f.endswith('.png') or f.endswith('.jpg')]
    mnames.sort()

    # å¦‚æœåªæœ‰ä¸€å¼ maskå›¾ç‰‡ï¼Œä¸”æŒ‡å®šäº†è§†é¢‘é•¿åº¦ï¼Œåˆ™å¤ç”¨è¿™å¼ å›¾ç‰‡
    if len(mnames) == 1 and video_length is not None:
        print(f"æ£€æµ‹åˆ°å•å¼ maskï¼Œå°†å¤ç”¨äºæ‰€æœ‰ {video_length} å¸§")
        m = Image.open(os.path.join(mpath, mnames[0]))
        m = m.resize((w, h), Image.NEAREST)
        m = np.array(m.convert('L'))
        m = np.array(m > 0).astype(np.uint8)
        m = cv2.dilate(m, cv2.getStructuringElement(
            cv2.MORPH_CROSS, (3, 3)), iterations=4)
        mask_img = Image.fromarray(m*255)
        # å¤ç”¨åŒä¸€å¼ å›¾ç‰‡
        masks = [mask_img] * video_length
    else:
        # é€å¸§è¯»å–
        for m in mnames:
            m = Image.open(os.path.join(mpath, m))
            m = m.resize((w, h), Image.NEAREST)
            m = np.array(m.convert('L'))
            m = np.array(m > 0).astype(np.uint8)
            m = cv2.dilate(m, cv2.getStructuringElement(
                cv2.MORPH_CROSS, (3, 3)), iterations=4)
            masks.append(Image.fromarray(m*255))

    return masks


#  read frames from video
def read_frame_from_videos(vname):
    frames = []
    vidcap = cv2.VideoCapture(vname)
    success, image = vidcap.read()
    count = 0
    while success:
        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        frames.append(image.resize((w,h)))
        success, image = vidcap.read()
        count += 1
    return frames


def calculate_crop_region(regions, video_width, video_height, padding=32):
    """
    æ ¹æ®regionsè®¡ç®—è£å‰ªåŒºåŸŸï¼ˆåŠ paddingï¼‰

    å‚æ•°:
        regions: åŒºåŸŸåˆ—è¡¨ [[left,bottom,right,top],...]ï¼Œç›¸å¯¹åæ ‡(0-1)
        video_width, video_height: åŸè§†é¢‘å°ºå¯¸
        padding: è¾¹ç•Œpaddingï¼ˆåƒç´ ï¼‰

    è¿”å›:
        (x, y, crop_w, crop_h): è£å‰ªå‚æ•°ï¼ˆåƒç´ åæ ‡ï¼‰
        (min_left, min_bottom, max_right, max_top): åˆå¹¶åçš„åŒºåŸŸè¾¹ç•Œï¼ˆç›¸å¯¹åæ ‡ï¼‰
    """
    # åˆå¹¶æ‰€æœ‰åŒºåŸŸçš„è¾¹ç•Œ
    all_lefts = [r[0] for r in regions]
    all_bottoms = [r[1] for r in regions]
    all_rights = [r[2] for r in regions]
    all_tops = [r[3] for r in regions]

    min_left = min(all_lefts)
    min_bottom = min(all_bottoms)
    max_right = max(all_rights)
    max_top = max(all_tops)

    # è½¬æ¢ä¸ºåƒç´ åæ ‡
    x1_pixel = int(min_left * video_width)
    x2_pixel = int(max_right * video_width)

    # Yåæ ‡ï¼šå·¦ä¸‹è§’åŸç‚¹ -> å·¦ä¸Šè§’åŸç‚¹
    y1_pixel = int((1 - max_top) * video_height)
    y2_pixel = int((1 - min_bottom) * video_height)

    # æ·»åŠ padding
    x1_pixel = max(0, x1_pixel - padding)
    y1_pixel = max(0, y1_pixel - padding)
    x2_pixel = min(video_width, x2_pixel + padding)
    y2_pixel = min(video_height, y2_pixel + padding)

    # è®¡ç®—è£å‰ªåŒºåŸŸå°ºå¯¸
    crop_w = x2_pixel - x1_pixel
    crop_h = y2_pixel - y1_pixel

    # ç¡®ä¿å°ºå¯¸æ˜¯å¶æ•°
    crop_w = crop_w if crop_w % 2 == 0 else crop_w - 1
    crop_h = crop_h if crop_h % 2 == 0 else crop_h - 1

    # ç¡®ä¿è£å‰ªå°ºå¯¸åŒ¹é…æ¨¡å‹patchsizeè¦æ±‚ï¼ˆ432x240çš„å€æ•°ï¼‰
    base_w, base_h = 432, 240

    # å‘ä¸‹å¯¹é½åˆ°432x240çš„å€æ•°
    w_mult = max(1, crop_w // base_w)
    h_mult = max(1, crop_h // base_h)

    aligned_w = base_w * w_mult
    aligned_h = base_h * h_mult

    # å¦‚æœå¤ªå°ï¼Œè‡³å°‘ç”¨1å€çš„åŸºç¡€å°ºå¯¸
    if aligned_w < base_w:
        aligned_w = base_w
    if aligned_h < base_h:
        aligned_h = base_h

    # è°ƒæ•´è£å‰ªåŒºåŸŸï¼ˆå±…ä¸­è£å‰ªï¼‰
    if aligned_w != crop_w or aligned_h != crop_h:
        w_diff = crop_w - aligned_w
        h_diff = crop_h - aligned_h

        # è°ƒæ•´èµ·å§‹ä½ç½®ï¼ˆä¿æŒå±…ä¸­ï¼‰
        x1_pixel += w_diff // 2
        y1_pixel += h_diff // 2

        # ç¡®ä¿ä¸è¶…å‡ºè¾¹ç•Œ
        x1_pixel = max(0, min(x1_pixel, video_width - aligned_w))
        y1_pixel = max(0, min(y1_pixel, video_height - aligned_h))

        crop_w = aligned_w
        crop_h = aligned_h

    return (x1_pixel, y1_pixel, crop_w, crop_h), (min_left, min_bottom, max_right, max_top)


def transform_regions_to_crop_space(regions, crop_region_bounds, video_width, video_height, crop_w, crop_h):
    """
    å°†å…¨å±€åæ ‡çš„regionsè½¬æ¢ä¸ºè£å‰ªç©ºé—´çš„ç›¸å¯¹åæ ‡

    å‚æ•°:
        regions: åŸå§‹åŒºåŸŸåˆ—è¡¨ï¼ˆå…¨å±€ç›¸å¯¹åæ ‡ï¼‰
        crop_region_bounds: (min_left, min_bottom, max_right, max_top) è£å‰ªåŒºåŸŸè¾¹ç•Œï¼ˆå…¨å±€ç›¸å¯¹åæ ‡ï¼‰
        video_width, video_height: åŸè§†é¢‘å°ºå¯¸
        crop_w, crop_h: è£å‰ªåçš„å°ºå¯¸

    è¿”å›:
        transformed_regions: è½¬æ¢åçš„åŒºåŸŸåˆ—è¡¨ï¼ˆè£å‰ªç©ºé—´ç›¸å¯¹åæ ‡ï¼‰
    """
    min_left, min_bottom, max_right, max_top = crop_region_bounds

    # è£å‰ªåŒºåŸŸçš„å°ºå¯¸ï¼ˆç›¸å¯¹åæ ‡ï¼‰
    crop_rel_width = max_right - min_left
    crop_rel_height = max_top - min_bottom

    transformed_regions = []
    for region in regions:
        left, bottom, right, top = region

        # è½¬æ¢åˆ°è£å‰ªç©ºé—´ï¼ˆç›¸å¯¹äºè£å‰ªåŒºåŸŸçš„å·¦ä¸‹è§’ï¼‰
        new_left = (left - min_left) / crop_rel_width
        new_right = (right - min_left) / crop_rel_width
        new_bottom = (bottom - min_bottom) / crop_rel_height
        new_top = (top - min_bottom) / crop_rel_height

        # ç¡®ä¿åœ¨[0,1]èŒƒå›´å†…
        new_left = max(0, min(1, new_left))
        new_right = max(0, min(1, new_right))
        new_bottom = max(0, min(1, new_bottom))
        new_top = max(0, min(1, new_top))

        transformed_regions.append([new_left, new_bottom, new_right, new_top])

    return transformed_regions


def crop_video_ffmpeg(input_video, output_video, x, y, width, height):
    """
    ä½¿ç”¨ffmpegè£å‰ªè§†é¢‘

    å‚æ•°:
        input_video: è¾“å…¥è§†é¢‘è·¯å¾„
        output_video: è¾“å‡ºè§†é¢‘è·¯å¾„
        x, y: è£å‰ªèµ·å§‹åæ ‡
        width, height: è£å‰ªå°ºå¯¸
    """
    cmd = [
        'ffmpeg', '-y', '-i', input_video,
        '-filter:v', f'crop={width}:{height}:{x}:{y}',
        '-c:a', 'copy',  # éŸ³é¢‘ç›´æ¥å¤åˆ¶
        output_video
    ]

    print(f"è£å‰ªå‘½ä»¤: {' '.join(cmd)}")
    result = subprocess.run(cmd, capture_output=True, text=True)

    if result.returncode != 0:
        raise RuntimeError(f"ffmpegè£å‰ªå¤±è´¥: {result.stderr}")

    return output_video


def merge_video_ffmpeg(original_video, inpainted_video, output_video, x, y):
    """
    ä½¿ç”¨ffmpegå°†ä¿®å¤åçš„è§†é¢‘åˆå¹¶å›åŸè§†é¢‘

    å‚æ•°:
        original_video: åŸå§‹è§†é¢‘è·¯å¾„
        inpainted_video: ä¿®å¤åçš„è£å‰ªè§†é¢‘è·¯å¾„
        output_video: è¾“å‡ºè§†é¢‘è·¯å¾„
        x, y: overlayä½ç½®
    """
    cmd = [
        'ffmpeg', '-y',
        '-i', original_video,
        '-i', inpainted_video,
        '-filter_complex', f'[0:v][1:v]overlay={x}:{y}',
        '-c:a', 'copy',  # ä¿ç•™åŸè§†é¢‘éŸ³é¢‘
        output_video
    ]

    print(f"åˆå¹¶å‘½ä»¤: {' '.join(cmd)}")
    result = subprocess.run(cmd, capture_output=True, text=True)

    if result.returncode != 0:
        raise RuntimeError(f"ffmpegåˆå¹¶å¤±è´¥: {result.stderr}")

    return output_video


def calculate_max_frames(width, height, gpu_memory_gb, safety_margin=0.5):
    """
    è®¡ç®—ç»™å®šæ˜¾å­˜ä¸‹å¯å¤„ç†çš„æœ€å¤§å¸§æ•°

    å‚æ•°:
        width, height: è§†é¢‘åˆ†è¾¨ç‡
        gpu_memory_gb: GPUæ˜¾å­˜å¤§å°(GB)
        safety_margin: å®‰å…¨ç³»æ•°(0-1)ï¼Œé»˜è®¤0.5ï¼ˆä¿å®ˆä¼°è®¡ï¼Œè€ƒè™‘ç¼–ç å™¨ä¸­é—´æ¿€æ´»ï¼‰

    è¿”å›:
        max_frames: æœ€å¤§å¸§æ•°
    """
    feat_h, feat_w = height // 4, width // 4

    # æ¯å¸§æ˜¾å­˜å ç”¨ï¼ˆé™æ€æ•°æ®ï¼‰
    per_frame_static = (
        256 * feat_h * feat_w * 4 +  # featsï¼ˆç¼–ç åï¼‰
        height * width * 4 +           # masks
        height * width * 3             # framesï¼ˆRGBï¼‰
    ) / (1024**3)

    # ç¼–ç å™¨ä¸­é—´æ¿€æ´»å€¼ï¼ˆä¸´æ—¶å ç”¨ï¼Œéå¸¸å¤§ï¼ï¼‰
    # Convå±‚ä¼šäº§ç”Ÿå¤šå±‚ä¸­é—´feature maps
    # ä¼°ç®—: input(3 ch) -> 64 -> 64 -> 128 -> 256
    encoder_temp_per_frame = (
        (3 + 64 + 64 + 128 + 256) * height * width * 4
    ) / (1024**3)

    # æ€»æ¯å¸§æ˜¾å­˜ = é™æ€ + ç¼–ç ä¸´æ—¶å¼€é”€çš„50%ï¼ˆå› ä¸ºä¸æ˜¯æ‰€æœ‰å±‚åŒæ—¶å­˜åœ¨ï¼‰
    per_frame_total = per_frame_static + encoder_temp_per_frame * 0.5

    overhead_gb = 2.0  # æ¨¡å‹æƒé‡(1.5GB) + transformerå¼€é”€(0.5GB)

    # å¯ç”¨æ˜¾å­˜ï¼ˆæ›´ä¿å®ˆï¼‰
    available_gb = gpu_memory_gb * safety_margin - overhead_gb

    if available_gb <= 0:
        print(f"âš ï¸  è­¦å‘Š: æ˜¾å­˜ä¸è¶³ä»¥å¤„ç†ï¼Œä½¿ç”¨æœ€å°å¸§æ•°")
        return 10

    max_frames = int(available_gb / per_frame_total)

    # è®¾ç½®åˆç†ä¸Šä¸‹é™
    max_frames = max(10, min(max_frames, 150))  # 10-150å¸§ä¹‹é—´

    return max_frames


def split_video_by_time(input_video, output_dir, segment_duration, fps):
    """
    æŒ‰æ—¶é—´åˆ†æ®µåˆ‡å‰²è§†é¢‘

    å‚æ•°:
        input_video: è¾“å…¥è§†é¢‘è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        segment_duration: æ¯æ®µæ—¶é•¿(ç§’)
        fps: è§†é¢‘å¸§ç‡

    è¿”å›:
        segment_files: åˆ†æ®µæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    os.makedirs(output_dir, exist_ok=True)

    # è·å–è§†é¢‘æ€»æ—¶é•¿
    cmd = [
        'ffprobe', '-v', 'error',
        '-show_entries', 'format=duration',
        '-of', 'default=noprint_wrappers=1:nokey=1',
        input_video
    ]
    result = subprocess.run(cmd, capture_output=True, text=True)
    total_duration = float(result.stdout.strip())

    print(f"è§†é¢‘æ€»æ—¶é•¿: {total_duration:.1f}ç§’")
    print(f"åˆ†æ®µæ—¶é•¿: {segment_duration:.1f}ç§’")

    segment_files = []
    segment_idx = 0
    start_time = 0

    while start_time < total_duration:
        output_file = os.path.join(output_dir, f"segment_{segment_idx:04d}.mp4")

        cmd = [
            'ffmpeg', '-y',
            '-ss', str(start_time),
            '-i', input_video,
            '-t', str(segment_duration),
            '-c', 'copy',
            output_file
        ]

        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode != 0:
            raise RuntimeError(f"è§†é¢‘åˆ†æ®µå¤±è´¥: {result.stderr}")

        segment_files.append(output_file)
        print(f"  âœ“ åˆ†æ®µ {segment_idx}: {start_time:.1f}s - {min(start_time + segment_duration, total_duration):.1f}s")

        start_time += segment_duration
        segment_idx += 1

    return segment_files


def concat_videos_ffmpeg(video_files, output_video):
    """
    ä½¿ç”¨ffmpegåˆå¹¶å¤šä¸ªè§†é¢‘

    å‚æ•°:
        video_files: è§†é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        output_video: è¾“å‡ºè§†é¢‘è·¯å¾„
    """
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶åˆ—è¡¨
    concat_list = output_video.replace('.mp4', '_concat.txt')

    with open(concat_list, 'w') as f:
        for video_file in video_files:
            # ä½¿ç”¨ç»å¯¹è·¯å¾„é¿å…é—®é¢˜
            abs_path = os.path.abspath(video_file)
            f.write(f"file '{abs_path}'\n")

    cmd = [
        'ffmpeg', '-y',
        '-f', 'concat',
        '-safe', '0',
        '-i', concat_list,
        '-c', 'copy',
        output_video
    ]

    print(f"åˆå¹¶å‘½ä»¤: {' '.join(cmd)}")
    result = subprocess.run(cmd, capture_output=True, text=True)

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶åˆ—è¡¨
    try:
        os.remove(concat_list)
    except:
        pass

    if result.returncode != 0:
        raise RuntimeError(f"è§†é¢‘åˆå¹¶å¤±è´¥: {result.stderr}")

    return output_video


def generate_masks_from_regions(video_path, regions, num_frames=None):
    """
    æ ¹æ®åŒºåŸŸåæ ‡ç”Ÿæˆmaskå›¾ç‰‡ï¼Œä¿å­˜åˆ°outputç›®å½•

    å‚æ•°:
        video_path: è§†é¢‘è·¯å¾„
        regions: åŒºåŸŸåˆ—è¡¨ï¼Œæ ¼å¼ä¸º [[left, bottom, right, top], ...]
                 åæ ‡ä¸ºç›¸å¯¹æ¯”ä¾‹ï¼ŒèŒƒå›´ 0-1ï¼Œä»¥å·¦ä¸‹è§’ä¸ºåŸç‚¹
                 left: å·¦è¾¹è·ç¦» (0=æœ€å·¦, 1=æœ€å³)
                 bottom: åº•è¾¹è·ç¦» (0=æœ€åº•, 1=æœ€é¡¶)
                 right: å³è¾¹è·ç¦» (0=æœ€å·¦, 1=æœ€å³)
                 top: é¡¶è¾¹è·ç¦» (0=æœ€åº•, 1=æœ€é¡¶)
        num_frames: å¸§æ•°ï¼ˆå¦‚æœä¸ºNoneåˆ™ä»è§†é¢‘ä¸­è·å–ï¼‰

    è¿”å›:
        mask_dir: maskç›®å½•è·¯å¾„
    """
    import datetime

    # è·å–è§†é¢‘ä¿¡æ¯
    vidcap = cv2.VideoCapture(video_path)

    if num_frames is None:
        num_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))

    # è·å–åŸå§‹è§†é¢‘å°ºå¯¸
    orig_width = int(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH))
    orig_height = int(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    vidcap.release()

    print(f"è§†é¢‘åŸå§‹å°ºå¯¸: {orig_width}x{orig_height}")
    print(f"å¤„ç†å°ºå¯¸(å…¨å±€w,h): {w}x{h}")
    print(f"è§†é¢‘æ€»å¸§æ•°: {num_frames}")
    print(f"åŒºåŸŸæ•°é‡: {len(regions)}")
    print(f"è¾“å…¥regions: {regions}")

    # åˆ›å»ºoutputç›®å½•ä¸‹çš„maskå­ç›®å½•
    output_base = "output"
    os.makedirs(output_base, exist_ok=True)

    # ä½¿ç”¨æ—¶é—´æˆ³åˆ›å»ºå”¯ä¸€çš„maskç›®å½•
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    video_name = os.path.splitext(os.path.basename(video_path))[0]
    mask_dir = os.path.join(output_base, f"masks_{video_name}_{timestamp}")
    os.makedirs(mask_dir, exist_ok=True)
    print(f"Maskç›®å½•: {mask_dir}")

    # åˆ›å»ºé»‘è‰²èƒŒæ™¯ (ä½¿ç”¨å¤„ç†å°ºå¯¸)
    mask = np.zeros((h, w), dtype=np.uint8)

    # åœ¨æŒ‡å®šåŒºåŸŸç»˜åˆ¶ç™½è‰²
    for region in regions:
        left, bottom, right, top = region

        # éªŒè¯åæ ‡èŒƒå›´
        if not (0 <= left < right <= 1 and 0 <= bottom < top <= 1):
            raise ValueError(f"æ— æ•ˆçš„åŒºåŸŸåæ ‡: {region}ï¼Œåæ ‡å¿…é¡»åœ¨[0,1]èŒƒå›´å†…ä¸”left<right, bottom<top")

        # è½¬æ¢ç›¸å¯¹åæ ‡åˆ°å®é™…åƒç´ åæ ‡ï¼ˆåŸºäºå¤„ç†å°ºå¯¸ï¼‰
        # æ³¨æ„ï¼šä»¥å·¦ä¸‹è§’ä¸ºåŸç‚¹ï¼Œéœ€è¦è½¬æ¢Yåæ ‡
        x1 = int(left * w)
        x2 = int(right * w)

        # Yåæ ‡è½¬æ¢ï¼šå·¦ä¸‹è§’åŸç‚¹ -> å·¦ä¸Šè§’åŸç‚¹ï¼ˆå›¾åƒåæ ‡ç³»ï¼‰
        # bottom=0 è¡¨ç¤ºæœ€åº•éƒ¨ï¼Œåœ¨å›¾åƒåæ ‡ç³»ä¸­æ˜¯ h
        # top=1 è¡¨ç¤ºæœ€é¡¶éƒ¨ï¼Œåœ¨å›¾åƒåæ ‡ç³»ä¸­æ˜¯ 0
        y1 = int((1 - top) * h)      # é¡¶éƒ¨åœ¨å›¾åƒåæ ‡ç³»ä¸­çš„ä½ç½®
        y2 = int((1 - bottom) * h)   # åº•éƒ¨åœ¨å›¾åƒåæ ‡ç³»ä¸­çš„ä½ç½®

        # ç¡®ä¿åæ ‡åœ¨æœ‰æ•ˆèŒƒå›´å†…
        x1 = max(0, min(x1, w - 1))
        x2 = max(0, min(x2, w))
        y1 = max(0, min(y1, h - 1))
        y2 = max(0, min(y2, h))

        # ç»˜åˆ¶ç™½è‰²åŒºåŸŸ
        mask[y1:y2, x1:x2] = 255

        print(f"  åŒºåŸŸ [left={left:.4f}, bottom={bottom:.4f}, right={right:.4f}, top={top:.4f}]")
        print(f"    -> åƒç´ åæ ‡: x=[{x1},{x2}] (å®½åº¦={x2-x1}), y=[{y1},{y2}] (é«˜åº¦={y2-y1})")
        print(f"    -> maskå°ºå¯¸: {mask.shape}, ç™½è‰²åƒç´ æ•°: {np.sum(mask == 255)}")

    # åº”ç”¨è†¨èƒ€æ“ä½œï¼ˆä¸read_maskä¿æŒä¸€è‡´ï¼‰
    mask = cv2.dilate(mask, cv2.getStructuringElement(
        cv2.MORPH_CROSS, (3, 3)), iterations=4)

    # åªä¿å­˜ä¸€å¼ maskå›¾ç‰‡
    mask_path = os.path.join(mask_dir, 'mask.png')
    Image.fromarray(mask).save(mask_path)

    print(f"âœ“ æˆåŠŸç”Ÿæˆå•å¼ mask (å°†å¤ç”¨äºæ‰€æœ‰ {num_frames} å¸§)")
    print(f"âœ“ èŠ‚çœç©ºé—´: ~{(num_frames - 1) * mask.nbytes / 1024 / 1024:.1f}MB")
    return mask_dir


def process_single_video(video_path, regions_to_use=None, output_path_override=None):
    """
    å¤„ç†å•ä¸ªè§†é¢‘ï¼ˆç”¨äºä¸»æµç¨‹å’Œåˆ†æ®µå¤„ç†ï¼‰

    å‚æ•°:
        video_path: è¾“å…¥è§†é¢‘è·¯å¾„
        regions_to_use: ä½¿ç”¨çš„regionsåæ ‡ï¼ˆå·²ç»è½¬æ¢å¥½çš„ï¼‰
        output_path_override: å¼ºåˆ¶æŒ‡å®šè¾“å‡ºè·¯å¾„ï¼ˆç”¨äºåˆ†æ®µå¤„ç†ï¼‰

    è¿”å›:
        output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
    """
    # æ³¨æ„ï¼šè£å‰ªé€»è¾‘å·²ç»åœ¨main_workerä¸­å¤„ç†ï¼Œè¿™é‡Œåªå¤„ç†å·²è£å‰ªçš„è§†é¢‘
    video_to_process = video_path

    # å¦‚æœæ²¡æœ‰ä¼ å…¥regionsï¼Œä»argsè·å–
    if regions_to_use is None and args.regions is not None:
        regions_to_use = json.loads(args.regions)

    # ===== ä»¥ä¸‹æ˜¯åŸæ¥çš„éè£å‰ªé€»è¾‘ï¼Œå»æ‰è£å‰ªéƒ¨åˆ† =====

    # æ—§çš„è£å‰ªé€»è¾‘å·²ç§»åˆ°main_worker
    if False:  # ä¿æŒç¼©è¿›ï¼Œä½†æ°¸è¿œä¸æ‰§è¡Œ
        print("\n" + "="*60)
        print("ğŸ¯ è£å‰ªæ¨¡å¼ï¼šä»…å¤„ç†æ“¦é™¤åŒºåŸŸå‘¨å›´éƒ¨åˆ†")
        print("="*60)

        regions = json.loads(args.regions)

        # è·å–åŸè§†é¢‘ä¿¡æ¯
        video_info = get_video_info_ffprobe(original_video)
        if video_info is None:
            video_info = get_video_info_opencv(original_video)
        orig_w, orig_h, orig_fps = video_info

        print(f"åŸè§†é¢‘å°ºå¯¸: {orig_w}x{orig_h}")
        print(f"æ“¦é™¤åŒºåŸŸ: {regions}")
        print(f"è¾¹ç•Œpadding: {args.crop_padding}px")

        # è®¡ç®—è£å‰ªåŒºåŸŸ
        (crop_x, crop_y, crop_w, crop_h), crop_bounds = calculate_crop_region(
            regions, orig_w, orig_h, padding=args.crop_padding
        )

        print(f"\nè£å‰ªå‚æ•°:")
        print(f"  ä½ç½®: x={crop_x}, y={crop_y}")
        print(f"  å°ºå¯¸: {crop_w}x{crop_h}")
        print(f"  æ˜¾å­˜é™ä½: ~{(orig_w*orig_h)/(crop_w*crop_h):.1f}x")

        # è½¬æ¢regionsåˆ°è£å‰ªç©ºé—´
        # æ³¨æ„ï¼šè¿™é‡Œcrop_w, crop_hå·²ç»æ˜¯å¯¹é½åçš„å°ºå¯¸ï¼Œéœ€è¦é‡æ–°è®¡ç®—crop_bounds
        # å› ä¸ºè£å‰ªåŒºåŸŸå¯èƒ½è¢«è°ƒæ•´è¿‡ï¼Œéœ€è¦åŸºäºå®é™…è£å‰ªå°ºå¯¸é‡æ–°è®¡ç®—è¾¹ç•Œ
        actual_crop_bounds = (
            crop_x / orig_w,  # min_left
            crop_y / orig_h,  # min_bottom (ä½†è¿™æ˜¯å›¾åƒåæ ‡ç³»ï¼Œéœ€è¦è½¬æ¢)
            (crop_x + crop_w) / orig_w,  # max_right
            (crop_y + crop_h) / orig_h   # max_top
        )

        # ä½†Yåæ ‡ç³»éœ€è¦è½¬æ¢ï¼ˆcrop_yæ˜¯å›¾åƒåæ ‡ç³»ï¼Œä»ä¸Šå¾€ä¸‹ï¼‰
        actual_crop_bounds = (
            crop_x / orig_w,  # min_left
            1 - (crop_y + crop_h) / orig_h,  # min_bottom (å·¦ä¸‹è§’åæ ‡ç³»)
            (crop_x + crop_w) / orig_w,  # max_right
            1 - crop_y / orig_h  # max_top (å·¦ä¸‹è§’åæ ‡ç³»)
        )

        transformed_regions = transform_regions_to_crop_space(
            regions, actual_crop_bounds, orig_w, orig_h, crop_w, crop_h
        )
        print(f"  å®é™…è£å‰ªè¾¹ç•Œ(å·¦ä¸‹è§’åæ ‡): {actual_crop_bounds}")
        print(f"  è½¬æ¢ååŒºåŸŸ: {transformed_regions}")

        # è£å‰ªè§†é¢‘
        print("\nè£å‰ªè§†é¢‘...")
        output_base = "output"
        os.makedirs(output_base, exist_ok=True)
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        cropped_video = os.path.join(output_base, f"cropped_{timestamp}.mp4")

        crop_video_ffmpeg(original_video, cropped_video, crop_x, crop_y, crop_w, crop_h)
        print(f"âœ“ è£å‰ªå®Œæˆ: {cropped_video}")

        # æ›´æ–°å¤„ç†ç›®æ ‡
        video_to_process = cropped_video
        regions_to_use = transformed_regions

        print("="*60 + "\n")

    # è®¾ç½®å¤„ç†åˆ†è¾¨ç‡
    print("\n" + "="*60)
    print("è§†é¢‘ä¿¡æ¯æ£€æµ‹")
    print("="*60)
    setup_resolution(video_to_process, args.resolution, args.scale, args.short_side)
    print("="*60 + "\n")

    # å…ˆç”Ÿæˆæˆ–å‡†å¤‡ maskï¼ˆåœ¨åŠ è½½æ¨¡å‹ä¹‹å‰ï¼‰
    temp_mask_dir = None  # ç”¨äºè·Ÿè¸ªä¸´æ—¶ç›®å½•
    if args.regions is not None:
        # ä»åŒºåŸŸåæ ‡ç”Ÿæˆmask
        print("="*60)
        print("ç”Ÿæˆ Mask")
        print("="*60)
        print(f"åŒºåŸŸåæ ‡: {regions_to_use}")

        # è·å–è§†é¢‘å¸§æ•°
        vidcap = cv2.VideoCapture(video_to_process)
        video_length = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))
        vidcap.release()

        temp_mask_dir = generate_masks_from_regions(video_to_process, regions_to_use, video_length)
        print("="*60 + "\n")
    else:
        # éªŒè¯maskç›®å½•å­˜åœ¨
        print("="*60)
        print("éªŒè¯ Mask ç›®å½•")
        print("="*60)
        if not os.path.exists(args.mask):
            raise ValueError(f"Maskç›®å½•ä¸å­˜åœ¨: {args.mask}")
        mask_files = [f for f in os.listdir(args.mask) if f.endswith('.png') or f.endswith('.jpg')]
        if not mask_files:
            raise ValueError(f"Maskç›®å½•ä¸­æ²¡æœ‰å›¾ç‰‡æ–‡ä»¶: {args.mask}")
        print(f"âœ“ Maskç›®å½•: {args.mask}")
        print(f"âœ“ æ‰¾åˆ° {len(mask_files)} ä¸ªmaskæ–‡ä»¶")
        print("="*60 + "\n")

    # åŠ è½½æ¨¡å‹
    print("="*60)
    print("åŠ è½½æ¨¡å‹")
    print("="*60)
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"âœ“ ä½¿ç”¨è®¾å¤‡: {device} (GPU: {torch.cuda.get_device_name(0)})")
    else:
        device = torch.device("cpu")
        print(f"âœ“ ä½¿ç”¨è®¾å¤‡: CPU")

    net = importlib.import_module('src.model.' + args.model)
    model = net.InpaintGenerator().to(device)
    model_path = args.ckpt
    data = torch.load(args.ckpt, map_location=device, weights_only=False)
    model.load_state_dict(data['netG'])
    print(f'âœ“ æ¨¡å‹åŠ è½½å®Œæˆ: {args.ckpt}')
    model.eval()
    print("="*60 + "\n")

    # åŠ è½½è§†é¢‘å¸§
    print("="*60)
    print("åŠ è½½è§†é¢‘å¸§")
    print("="*60)
    frames = read_frame_from_videos(video_to_process)
    video_length = len(frames)
    print(f"âœ“ åŠ è½½ {video_length} å¸§")

    # é‡æ–°ä¼°ç®—æ˜¾å­˜ï¼ˆç°åœ¨çŸ¥é“å¸§æ•°äº†ï¼‰
    print("\né‡æ–°ä¼°ç®—æ˜¾å­˜éœ€æ±‚:")
    estimate_memory(w, h, video_length)
    print()

    feats = _to_tensors(frames).unsqueeze(0)*2-1
    frames = [np.array(f).astype(np.uint8) for f in frames]
    print("="*60 + "\n")

    # åŠ è½½mask
    print("="*60)
    print("åŠ è½½ Mask")
    print("="*60)
    if temp_mask_dir is not None:
        masks = read_mask(temp_mask_dir, video_length)
    else:
        masks = read_mask(args.mask, video_length)
    print(f"âœ“ åŠ è½½ {len(masks)} ä¸ªmask")
    print("="*60 + "\n")

    binary_masks = [np.expand_dims((np.array(m) != 0).astype(np.uint8), 2) for m in masks]
    masks = _to_tensors(masks).unsqueeze(0)

    # æ˜¾ç¤ºæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
    if torch.cuda.is_available():
        print("æ˜¾å­˜çŠ¶æ€ (åŠ è½½å‰):")
        torch.cuda.empty_cache()
        allocated = torch.cuda.memory_allocated(0) / (1024**3)
        print(f"  å·²åˆ†é…: {allocated:.2f}GB")

    # åˆ†æ­¥åŠ è½½åˆ°GPUï¼Œç›‘æ§æ˜¾å­˜
    print("\nåŠ è½½æ•°æ®åˆ°GPU...")
    try:
        feats = feats.to(device)
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / (1024**3)
            print(f"  featsåŠ è½½å: {allocated:.2f}GB")

        masks = masks.to(device)
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / (1024**3)
            print(f"  masksåŠ è½½å: {allocated:.2f}GB")
    except torch.cuda.OutOfMemoryError as e:
        print(f"\nâŒ æ˜¾å­˜ä¸è¶³ï¼")
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / (1024**3)
            total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            print(f"   å·²ç”¨: {allocated:.2f}GB / {total:.1f}GB")
        print(f"\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print(f"   1. ç¼©çŸ­è§†é¢‘: ffmpeg -i input.mp4 -t 10 output.mp4  (å‰10ç§’)")
        print(f"   2. é™ä½åˆ†è¾¨ç‡: --short-side 360")
        print(f"   3. ä½¿ç”¨è£å‰ªæ¨¡å¼: --crop (å¼ºçƒˆæ¨èï¼)")
        raise

    comp_frames = [None]*video_length

    # ç¼–ç ç‰¹å¾
    print("="*60)
    print("ç¼–ç è§†é¢‘ç‰¹å¾")
    print("="*60)

    # é‡Šæ”¾ä¸å¿…è¦çš„æ˜¾å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    with torch.no_grad():
        try:
            feats = model.encoder((feats*(1-masks).float()).view(video_length, 3, h, w))
            _, c, feat_h, feat_w = feats.size()
            feats = feats.view(1, video_length, c, feat_h, feat_w)
        except torch.cuda.OutOfMemoryError as e:
            print(f"\nâŒ ç¼–ç é˜¶æ®µæ˜¾å­˜ä¸è¶³ï¼")
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(0) / (1024**3)
                total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                print(f"   å·²ç”¨: {allocated:.2f}GB / {total:.1f}GB")
            print(f"\nå½“å‰ç‰‡æ®µå¸§æ•°: {video_length}")
            print(f"å»ºè®®ä½¿ç”¨ --max-frames {video_length // 2} å‡å°‘æ¯æ®µå¸§æ•°")
            raise

    print(f'âœ“ ç‰¹å¾ç¼–ç å®Œæˆ: {video_length} å¸§')

    # ç¼–ç å®Œæˆåé‡Šæ”¾æ˜¾å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        allocated = torch.cuda.memory_allocated(0) / (1024**3)
        print(f'  ç¼–ç åæ˜¾å­˜: {allocated:.2f}GB')

    print("="*60 + "\n")

    # ä¿®å¤è§†é¢‘
    print("="*60)
    print("å¼€å§‹ä¿®å¤è§†é¢‘")
    print("="*60)
    total_steps = (video_length + neighbor_stride - 1) // neighbor_stride
    for step, f in enumerate(range(0, video_length, neighbor_stride), 1):
        neighbor_ids = [i for i in range(max(0, f-neighbor_stride), min(video_length, f+neighbor_stride+1))]
        ref_ids = get_ref_index(neighbor_ids, video_length)

        print(f"å¤„ç†è¿›åº¦: {step}/{total_steps} (å¸§ {f}-{min(f+neighbor_stride, video_length)}/{video_length})", end='\r')

        with torch.no_grad():
            pred_feat = model.infer(
                feats[0, neighbor_ids+ref_ids, :, :, :], masks[0, neighbor_ids+ref_ids, :, :, :])
            pred_img = torch.tanh(model.decoder(
                pred_feat[:len(neighbor_ids), :, :, :])).detach()
            pred_img = (pred_img + 1) / 2
            pred_img = pred_img.cpu().permute(0, 2, 3, 1).numpy()*255
            for i in range(len(neighbor_ids)):
                idx = neighbor_ids[i]
                img = np.array(pred_img[i]).astype(
                    np.uint8)*binary_masks[idx] + frames[idx] * (1-binary_masks[idx])
                if comp_frames[idx] is None:
                    comp_frames[idx] = img
                else:
                    comp_frames[idx] = comp_frames[idx].astype(
                        np.float32)*0.5 + img.astype(np.float32)*0.5
    print()  # æ¢è¡Œ
    print(f'âœ“ è§†é¢‘ä¿®å¤å®Œæˆ')
    print("="*60 + "\n")

    # ç¡®å®šè¾“å‡ºè·¯å¾„
    if output_path_override:
        final_output_path = output_path_override
    elif args.output:
        final_output_path = args.output
    else:
        # é»˜è®¤è¾“å‡ºåˆ°outputç›®å½•
        output_base = "output"
        os.makedirs(output_base, exist_ok=True)

        if args.mask:
            # ä½¿ç”¨maskç›®å½•åç§°
            mask_basename = os.path.basename(args.mask.rstrip('/'))
            final_output_path = os.path.join(output_base, f"{mask_basename}_result.mp4")
        else:
            # ä½¿ç”¨è§†é¢‘åç§°
            video_name = os.path.splitext(os.path.basename(video_path))[0]
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            final_output_path = os.path.join(output_base, f"{video_name}_inpainted_{timestamp}.mp4")

    # ä¿å­˜ä¿®å¤åçš„è§†é¢‘ï¼ˆç®€åŒ–ç‰ˆï¼Œä¸å¤„ç†overlayï¼‰
    print("="*60)
    print("ä¿å­˜è¾“å‡ºè§†é¢‘")
    print("="*60)
    print(f"è¾“å‡ºè·¯å¾„: {final_output_path}")

    writer = cv2.VideoWriter(final_output_path, cv2.VideoWriter_fourcc(*"mp4v"), default_fps, (w, h))
    for f in range(video_length):
        comp = np.array(comp_frames[f]).astype(
            np.uint8)*binary_masks[f] + frames[f] * (1-binary_masks[f])
        writer.write(cv2.cvtColor(np.array(comp).astype(np.uint8), cv2.COLOR_BGR2RGB))
        print(f"å†™å…¥å¸§: {f+1}/{video_length}", end='\r')
    writer.release()
    print()  # æ¢è¡Œ
    print(f'âœ“ è§†é¢‘ä¿å­˜å®Œæˆ: {final_output_path}')
    print("="*60 + "\n")

    # ä¿ç•™maskç›®å½•ä¾›ç”¨æˆ·æŸ¥çœ‹
    if temp_mask_dir is not None:
        print(f'âœ“ Maskæ–‡ä»¶ä¿å­˜åœ¨: {temp_mask_dir}')

    print("="*60)
    print("ğŸ‰ å…¨éƒ¨å®Œæˆï¼")
    print(f"ğŸ“¹ æœ€ç»ˆè¾“å‡º: {final_output_path}")
    print("="*60)

    return final_output_path


def main_worker():
    """ä¸»å·¥ä½œæµç¨‹ï¼šå¤„ç†å®Œæ•´è§†é¢‘æˆ–è‡ªåŠ¨åˆ†æ®µå¤„ç†"""

    # éªŒè¯å‚æ•°
    if args.mask is None and args.regions is None:
        raise ValueError("å¿…é¡»æŒ‡å®š --mask æˆ– --regions å‚æ•°ä¹‹ä¸€")
    if args.mask is not None and args.regions is not None:
        raise ValueError("--mask å’Œ --regions å‚æ•°ä¸èƒ½åŒæ—¶ä½¿ç”¨")
    if args.crop and args.regions is None:
        raise ValueError("--crop æ¨¡å¼å¿…é¡»é…åˆ --regions ä½¿ç”¨")

    # ========== ç¬¬1æ­¥ï¼šè£å‰ªåŸè§†é¢‘ï¼ˆå¦‚æœéœ€è¦ï¼‰ ==========
    crop_mode = args.crop and args.regions is not None
    cropped_video = None
    crop_x, crop_y = 0, 0
    original_video = args.video
    transformed_regions = None

    if crop_mode:
        print("\n" + "="*60)
        print("ğŸ¯ æ­¥éª¤1: è£å‰ªåŸè§†é¢‘")
        print("="*60)

        regions = json.loads(args.regions)

        # è·å–åŸè§†é¢‘ä¿¡æ¯
        video_info = get_video_info_ffprobe(original_video)
        if video_info is None:
            video_info = get_video_info_opencv(original_video)
        orig_w, orig_h, orig_fps = video_info

        print(f"åŸè§†é¢‘å°ºå¯¸: {orig_w}x{orig_h}")
        print(f"æ“¦é™¤åŒºåŸŸ: {regions}")
        print(f"è¾¹ç•Œpadding: {args.crop_padding}px")

        # è®¡ç®—è£å‰ªåŒºåŸŸ
        (crop_x, crop_y, crop_w, crop_h), crop_bounds = calculate_crop_region(
            regions, orig_w, orig_h, padding=args.crop_padding
        )

        print(f"\nè£å‰ªå‚æ•°:")
        print(f"  ä½ç½®: x={crop_x}, y={crop_y}")
        print(f"  å°ºå¯¸: {crop_w}x{crop_h}")
        print(f"  æ˜¾å­˜é™ä½: ~{(orig_w*orig_h)/(crop_w*crop_h):.1f}x")

        # è½¬æ¢regionsåˆ°è£å‰ªç©ºé—´
        actual_crop_bounds = (
            crop_x / orig_w,
            1 - (crop_y + crop_h) / orig_h,
            (crop_x + crop_w) / orig_w,
            1 - crop_y / orig_h
        )

        transformed_regions = transform_regions_to_crop_space(
            regions, actual_crop_bounds, orig_w, orig_h, crop_w, crop_h
        )
        print(f"  å®é™…è£å‰ªè¾¹ç•Œ: {actual_crop_bounds}")
        print(f"  è½¬æ¢ååŒºåŸŸ: {transformed_regions}")

        # è£å‰ªè§†é¢‘
        print("\nè£å‰ªè§†é¢‘...")
        output_base = "output"
        os.makedirs(output_base, exist_ok=True)
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        cropped_video = os.path.join(output_base, f"cropped_{timestamp}.mp4")

        crop_video_ffmpeg(original_video, cropped_video, crop_x, crop_y, crop_w, crop_h)
        print(f"âœ“ è£å‰ªå®Œæˆ: {cropped_video}")
        print("="*60 + "\n")

        # æ›´æ–°å¤„ç†ç›®æ ‡
        video_to_process = cropped_video
        regions_for_processing = transformed_regions
    else:
        video_to_process = original_video
        regions_for_processing = json.loads(args.regions) if args.regions else None

    # ========== ç¬¬2æ­¥ï¼šè‡ªåŠ¨åˆ†æ®µå¤„ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰ ==========
    if args.auto_split:
        print("\n" + "="*60)
        print("ğŸ”„ æ­¥éª¤2: è‡ªåŠ¨åˆ†æ®µå¤„ç†")
        print("="*60)

        # è·å–å¾…å¤„ç†è§†é¢‘ä¿¡æ¯ï¼ˆå·²è£å‰ªçš„æˆ–åŸè§†é¢‘ï¼‰
        video_info = get_video_info_ffprobe(video_to_process)
        if video_info is None:
            video_info = get_video_info_opencv(video_to_process)
        video_w, video_h, video_fps = video_info

        # è·å–è§†é¢‘å¸§æ•°
        vidcap = cv2.VideoCapture(video_to_process)
        total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))
        vidcap.release()

        print(f"å¾…å¤„ç†è§†é¢‘: {video_to_process}")
        print(f"è§†é¢‘ä¿¡æ¯: {video_w}x{video_h}, {total_frames}å¸§, {video_fps}fps")

        # è®¡ç®—æ¯æ®µæœ€å¤§å¸§æ•°
        if args.max_frames:
            max_frames_per_segment = args.max_frames
            print(f"ä½¿ç”¨æŒ‡å®šçš„æœ€å¤§å¸§æ•°: {max_frames_per_segment}")
        else:
            # è‡ªåŠ¨è®¡ç®—ï¼ˆåŸºäºå½“å‰è§†é¢‘å°ºå¯¸ï¼‰
            if torch.cuda.is_available():
                gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            else:
                gpu_memory_gb = 8  # CPUæ¨¡å¼é»˜è®¤

            max_frames_per_segment = calculate_max_frames(video_w, video_h, gpu_memory_gb)
            print(f"GPUæ˜¾å­˜: {gpu_memory_gb:.1f}GB")
            print(f"è‡ªåŠ¨è®¡ç®—æœ€å¤§å¸§æ•°: {max_frames_per_segment}å¸§ (~{max_frames_per_segment/video_fps:.1f}ç§’)")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†æ®µ
        if total_frames <= max_frames_per_segment:
            print(f"\nâœ“ è§†é¢‘è¾ƒçŸ­ï¼Œæ— éœ€åˆ†æ®µï¼Œç›´æ¥å¤„ç†")
            print("="*60 + "\n")
            result = process_single_video(video_to_process, regions_to_use=regions_for_processing)

            # å¦‚æœæ˜¯è£å‰ªæ¨¡å¼ï¼Œéœ€è¦overlayå›åŸè§†é¢‘
            if crop_mode:
                print("\n" + "="*60)
                print("æ­¥éª¤3: åˆå¹¶åˆ°åŸè§†é¢‘")
                print("="*60)
                final_output = args.output if args.output else os.path.join("output", f"{os.path.splitext(os.path.basename(original_video))[0]}_inpainted_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4")

                merge_video_ffmpeg(original_video, result, final_output, crop_x, crop_y)
                print(f"âœ“ åˆå¹¶å®Œæˆ: {final_output}")

                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    if os.path.exists(cropped_video):
                        os.remove(cropped_video)
                    if os.path.exists(result):
                        os.remove(result)
                except:
                    pass

                print("="*60)
            return

        # éœ€è¦åˆ†æ®µå¤„ç†
        num_segments = (total_frames + max_frames_per_segment - 1) // max_frames_per_segment
        segment_duration = max_frames_per_segment / video_fps

        print(f"\néœ€è¦åˆ†æ®µå¤„ç†:")
        print(f"  æ€»å¸§æ•°: {total_frames}")
        print(f"  æ¯æ®µå¸§æ•°: {max_frames_per_segment}")
        print(f"  åˆ†æ®µæ•°é‡: {num_segments}")
        print(f"  æ¯æ®µæ—¶é•¿: ~{segment_duration:.1f}ç§’")
        print("="*60 + "\n")

        # åˆ›å»ºä¸´æ—¶ç›®å½•
        output_base = "output"
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        temp_dir = os.path.join(output_base, f"temp_segments_{timestamp}")
        os.makedirs(temp_dir, exist_ok=True)

        # åˆ†æ®µåˆ‡å‰²è§†é¢‘ï¼ˆåˆ‡å‰²å·²è£å‰ªçš„è§†é¢‘ï¼‰
        print("="*60)
        print("åˆ†æ®µåˆ‡å‰²è§†é¢‘")
        print("="*60)
        segment_files = split_video_by_time(video_to_process, temp_dir, segment_duration, video_fps)
        print(f"âœ“ å®Œæˆåˆ†æ®µåˆ‡å‰²: {len(segment_files)} ä¸ªç‰‡æ®µ")
        print("="*60 + "\n")

        # é€æ®µå¤„ç†
        processed_files = []
        for idx, segment_file in enumerate(segment_files, 1):
            print("\n" + "="*60)
            print(f"å¤„ç†ç‰‡æ®µ {idx}/{len(segment_files)}")
            print("="*60)

            segment_output = os.path.join(temp_dir, f"processed_{idx:04d}.mp4")

            try:
                process_single_video(segment_file, regions_to_use=regions_for_processing, output_path_override=segment_output)
                processed_files.append(segment_output)
                print(f"âœ“ ç‰‡æ®µ {idx} å¤„ç†å®Œæˆ")
            except Exception as e:
                print(f"âŒ ç‰‡æ®µ {idx} å¤„ç†å¤±è´¥: {e}")
                raise

        # åˆå¹¶æ‰€æœ‰ç‰‡æ®µ
        print("\n" + "="*60)
        print("åˆå¹¶æ‰€æœ‰ç‰‡æ®µ")
        print("="*60)

        # åˆå¹¶åçš„ä¸´æ—¶è¾“å‡º
        merged_output = os.path.join(temp_dir, "merged_result.mp4")
        concat_videos_ffmpeg(processed_files, merged_output)
        print(f"âœ“ ç‰‡æ®µåˆå¹¶å®Œæˆ: {merged_output}")
        print("="*60 + "\n")

        # ========== ç¬¬3æ­¥ï¼šå¦‚æœæ˜¯è£å‰ªæ¨¡å¼ï¼Œoverlayå›åŸè§†é¢‘ ==========
        if crop_mode:
            print("="*60)
            print("æ­¥éª¤3: åˆå¹¶åˆ°åŸè§†é¢‘")
            print("="*60)

            if args.output:
                final_output = args.output
            else:
                video_name = os.path.splitext(os.path.basename(original_video))[0]
                final_output = os.path.join(output_base, f"{video_name}_inpainted_{timestamp}.mp4")

            print(f"åŸè§†é¢‘: {original_video}")
            print(f"ä¿®å¤åŒºåŸŸ: {merged_output}")
            print(f"æœ€ç»ˆè¾“å‡º: {final_output}")
            print(f"Overlayä½ç½®: x={crop_x}, y={crop_y}")

            merge_video_ffmpeg(original_video, merged_output, final_output, crop_x, crop_y)
            print(f"âœ“ åˆå¹¶å®Œæˆ: {final_output}")
            print("="*60 + "\n")
        else:
            # éè£å‰ªæ¨¡å¼ï¼Œç›´æ¥ä½œä¸ºæœ€ç»ˆè¾“å‡º
            if args.output:
                final_output = args.output
            else:
                video_name = os.path.splitext(os.path.basename(original_video))[0]
                final_output = os.path.join(output_base, f"{video_name}_inpainted_{timestamp}.mp4")

            # ç§»åŠ¨merged_outputåˆ°final_output
            import shutil
            shutil.move(merged_output, final_output)
            print(f"âœ“ è¾“å‡º: {final_output}")
            print("="*60 + "\n")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        print("="*60)
        print("æ¸…ç†ä¸´æ—¶æ–‡ä»¶")
        print("="*60)
        try:
            for segment_file in segment_files:
                if os.path.exists(segment_file):
                    os.remove(segment_file)
                    print(f"âœ“ åˆ é™¤åˆ†æ®µ: {os.path.basename(segment_file)}")

            for processed_file in processed_files:
                if os.path.exists(processed_file):
                    os.remove(processed_file)
                    print(f"âœ“ åˆ é™¤å¤„ç†ç»“æœ: {os.path.basename(processed_file)}")

            # å¦‚æœæ˜¯è£å‰ªæ¨¡å¼ï¼Œåˆ é™¤è£å‰ªçš„ä¸­é—´è§†é¢‘
            if crop_mode and cropped_video and os.path.exists(cropped_video):
                os.remove(cropped_video)
                print(f"âœ“ åˆ é™¤è£å‰ªè§†é¢‘: {os.path.basename(cropped_video)}")

            # åˆ é™¤ä¸´æ—¶ç›®å½•
            if os.path.exists(temp_dir) and not os.listdir(temp_dir):
                os.rmdir(temp_dir)
                print(f"âœ“ åˆ é™¤ä¸´æ—¶ç›®å½•: {os.path.basename(temp_dir)}")
        except Exception as e:
            print(f"âš ï¸  æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

        print("="*60 + "\n")

        print("="*60)
        print("ğŸ‰ å…¨éƒ¨å®Œæˆï¼")
        print(f"ğŸ“¹ æœ€ç»ˆè¾“å‡º: {final_output}")
        print(f"ğŸ“Š å¤„ç†äº† {len(segment_files)} ä¸ªç‰‡æ®µï¼Œå…± {total_frames} å¸§")
        print("="*60)
    else:
        # æ­£å¸¸å¤„ç†ï¼ˆä¸åˆ†æ®µï¼Œä½†å¯èƒ½æœ‰è£å‰ªï¼‰
        result = process_single_video(video_to_process, regions_to_use=regions_for_processing)

        # å¦‚æœæ˜¯è£å‰ªæ¨¡å¼ï¼Œéœ€è¦overlayå›åŸè§†é¢‘
        if crop_mode:
            print("\n" + "="*60)
            print("åˆå¹¶åˆ°åŸè§†é¢‘")
            print("="*60)

            if args.output:
                final_output = args.output
            else:
                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                video_name = os.path.splitext(os.path.basename(original_video))[0]
                final_output = os.path.join("output", f"{video_name}_inpainted_{timestamp}.mp4")

            print(f"åŸè§†é¢‘: {original_video}")
            print(f"ä¿®å¤åŒºåŸŸ: {result}")
            print(f"æœ€ç»ˆè¾“å‡º: {final_output}")
            print(f"Overlayä½ç½®: x={crop_x}, y={crop_y}")

            merge_video_ffmpeg(original_video, result, final_output, crop_x, crop_y)
            print(f"âœ“ åˆå¹¶å®Œæˆ: {final_output}")

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                if os.path.exists(cropped_video):
                    os.remove(cropped_video)
                if os.path.exists(result):
                    os.remove(result)
            except:
                pass

            print("="*60 + "\n")
            print("="*60)
            print("ğŸ‰ å…¨éƒ¨å®Œæˆï¼")
            print(f"ğŸ“¹ æœ€ç»ˆè¾“å‡º: {final_output}")
            print("="*60)


if __name__ == '__main__':
    main_worker()

# -*- coding: utf-8 -*-
import cv2
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
import math
import time
import importlib
import os
import argparse
import copy
import datetime
import random
import sys
import json
import subprocess

import torch
from torch.autograd import Variable

import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init
import torch.utils.model_zoo as model_zoo
from torchvision import models
import torch.multiprocessing as mp
from torchvision import transforms

# My libs
from src.core.utils import Stack, ToTorchFormatTensor


parser = argparse.ArgumentParser(description="STTN")
parser.add_argument("-v", "--video", type=str, required=True, help="è¾“å…¥è§†é¢‘è·¯å¾„")
parser.add_argument("-m", "--mask",   type=str, help="maskå›¾ç‰‡ç›®å½•è·¯å¾„ï¼ˆä¸--regionsäºŒé€‰ä¸€ï¼‰")
parser.add_argument("-c", "--ckpt",   type=str, required=True, help="æ¨¡å‹checkpointè·¯å¾„")
parser.add_argument("--model",   type=str, default='sttn', help="æ¨¡å‹åç§°")
parser.add_argument("--regions", type=str, help="éœ€è¦æ“¦é™¤çš„åŒºåŸŸåæ ‡ï¼Œæ ¼å¼ï¼š[[left,bottom,right,top],...]ï¼Œåæ ‡ä¸ºç›¸å¯¹æ¯”ä¾‹(0-1)ï¼ŒåŸç‚¹åœ¨å·¦ä¸‹è§’")
parser.add_argument("--output", type=str, help="è¾“å‡ºè§†é¢‘è·¯å¾„ï¼ˆé»˜è®¤ï¼š{mask}_result.mp4ï¼‰")
parser.add_argument("--resolution", type=str, help="å¤„ç†åˆ†è¾¨ç‡ï¼Œæ ¼å¼ï¼šWxH (å¦‚ 1280x720)ï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹")
parser.add_argument("--scale", type=float, default=1.0, help="åˆ†è¾¨ç‡ç¼©æ”¾æ¯”ä¾‹ (0-1]ï¼Œé»˜è®¤1.0ä¿æŒåŸåˆ†è¾¨ç‡")
parser.add_argument("--short-side", type=int, choices=[270, 360, 480, 540, 720, 1080],
                    help="ç­‰æ¯”ç¼©æ”¾åˆ°æŒ‡å®šçŸ­è¾¹å°ºå¯¸ï¼Œä¿æŒå®½é«˜æ¯” (270/360/480/540/720/1080)")
args = parser.parse_args()


# é»˜è®¤å€¼ï¼Œå°†åœ¨ get_video_info ä¸­æ›´æ–°
w, h = 432, 240
ref_length = 10
neighbor_stride = 5
default_fps = 24

_to_tensors = transforms.Compose([
    Stack(),
    ToTorchFormatTensor()])


def get_video_info_ffprobe(video_path):
    """
    ä½¿ç”¨ ffprobe è·å–è§†é¢‘ä¿¡æ¯

    è¿”å›: (width, height, fps) æˆ– None
    """
    try:
        cmd = [
            'ffprobe',
            '-v', 'error',
            '-select_streams', 'v:0',
            '-show_entries', 'stream=width,height,r_frame_rate',
            '-of', 'json',
            video_path
        ]

        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)

        if result.returncode == 0:
            data = json.loads(result.stdout)
            if 'streams' in data and len(data['streams']) > 0:
                stream = data['streams'][0]
                width = stream.get('width')
                height = stream.get('height')

                # è§£æå¸§ç‡ (æ ¼å¼å¯èƒ½æ˜¯ "30/1" æˆ– "30000/1001")
                fps_str = stream.get('r_frame_rate', '24/1')
                if '/' in fps_str:
                    num, den = map(int, fps_str.split('/'))
                    fps = num / den if den != 0 else 24
                else:
                    fps = float(fps_str)

                return width, height, fps
    except (subprocess.TimeoutExpired, FileNotFoundError, json.JSONDecodeError) as e:
        print(f"âš ï¸  ffprobe è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}")

    return None


def get_video_info_opencv(video_path):
    """
    ä½¿ç”¨ OpenCV è·å–è§†é¢‘ä¿¡æ¯ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰

    è¿”å›: (width, height, fps)
    """
    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")

    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)

    cap.release()

    # å¦‚æœ fps æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼
    if fps <= 0 or fps > 120:
        fps = 24

    return width, height, fps


def setup_resolution(video_path, resolution_arg=None, scale=1.0, short_side=None):
    """
    è®¾ç½®å¤„ç†åˆ†è¾¨ç‡

    å‚æ•°:
        video_path: è§†é¢‘è·¯å¾„
        resolution_arg: ç”¨æˆ·æŒ‡å®šçš„åˆ†è¾¨ç‡å­—ç¬¦ä¸² (å¦‚ "1280x720")
        scale: ç¼©æ”¾æ¯”ä¾‹
        short_side: çŸ­è¾¹ç›®æ ‡å°ºå¯¸ï¼ˆç­‰æ¯”ç¼©æ”¾ï¼‰

    è¿”å›: (width, height, fps)
    """
    global w, h, default_fps

    # 1. å¦‚æœç”¨æˆ·æŒ‡å®šäº†åˆ†è¾¨ç‡ï¼Œç›´æ¥ä½¿ç”¨
    if resolution_arg:
        try:
            w, h = map(int, resolution_arg.lower().split('x'))
            print(f"âœ“ ä½¿ç”¨æŒ‡å®šåˆ†è¾¨ç‡: {w}x{h}")
        except ValueError:
            print(f"âš ï¸  æ— æ•ˆçš„åˆ†è¾¨ç‡æ ¼å¼: {resolution_arg}ï¼Œå°†è‡ªåŠ¨æ£€æµ‹")
            resolution_arg = None

    # 2. è·å–è§†é¢‘ä¿¡æ¯
    if not resolution_arg:
        # ä¼˜å…ˆä½¿ç”¨ ffprobe
        video_info = get_video_info_ffprobe(video_path)

        # å¦‚æœ ffprobe å¤±è´¥ï¼Œä½¿ç”¨ OpenCV
        if video_info is None:
            print("âš ï¸  ffprobe ä¸å¯ç”¨ï¼Œä½¿ç”¨ OpenCV è·å–è§†é¢‘ä¿¡æ¯")
            video_info = get_video_info_opencv(video_path)

        orig_w, orig_h, fps = video_info

        # åº”ç”¨çŸ­è¾¹ç¼©æ”¾ï¼ˆä¼˜å…ˆçº§é«˜äº scaleï¼‰
        if short_side:
            current_short = min(orig_w, orig_h)
            current_long = max(orig_w, orig_h)

            # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
            scale_ratio = short_side / current_short

            # åº”ç”¨ç¼©æ”¾
            if orig_w < orig_h:  # ç«–å±
                w = short_side
                h = int(current_long * scale_ratio)
            else:  # æ¨ªå±
                h = short_side
                w = int(current_long * scale_ratio)

            print(f"âœ“ åŸå§‹åˆ†è¾¨ç‡: {orig_w}x{orig_h}")
            print(f"âœ“ ç­‰æ¯”ç¼©æ”¾åˆ°çŸ­è¾¹ {short_side}: {w}x{h}")
        # åº”ç”¨ç¼©æ”¾æ¯”ä¾‹
        elif scale != 1.0:
            if scale <= 0 or scale > 1:
                print(f"âš ï¸  æ— æ•ˆçš„ç¼©æ”¾æ¯”ä¾‹ {scale}ï¼Œä½¿ç”¨ 1.0")
                scale = 1.0
            w = int(orig_w * scale)
            h = int(orig_h * scale)
            print(f"âœ“ åŸå§‹åˆ†è¾¨ç‡: {orig_w}x{orig_h}")
            print(f"âœ“ ç¼©æ”¾æ¯”ä¾‹: {scale}")
            print(f"âœ“ å¤„ç†åˆ†è¾¨ç‡: {w}x{h}")
        else:
            w = orig_w
            h = orig_h
            print(f"âœ“ è‡ªåŠ¨æ£€æµ‹åˆ†è¾¨ç‡: {w}x{h}")

        # æ›´æ–°å¸§ç‡
        default_fps = int(fps)
        print(f"âœ“ æ£€æµ‹åˆ°å¸§ç‡: {default_fps} fps")

    # 3. ç¡®ä¿åˆ†è¾¨ç‡æ˜¯å¶æ•°ï¼ˆè§†é¢‘ç¼–ç è¦æ±‚ï¼‰
    w = w if w % 2 == 0 else w + 1
    h = h if h % 2 == 0 else h + 1

    # 4. æ˜¾ç¤ºæ˜¾å­˜ä¼°ç®—
    estimate_memory(w, h)

    return w, h, default_fps


def estimate_memory(width, height):
    """
    ä¼°ç®—æ‰€éœ€æ˜¾å­˜
    """
    pixels = width * height
    # ç²—ç•¥ä¼°ç®—ï¼šæ¯ç™¾ä¸‡åƒç´ çº¦éœ€ 6GB æ˜¾å­˜
    estimated_gb = (pixels / 1_000_000) * 6

    print(f"ğŸ“Š åˆ†è¾¨ç‡ä¿¡æ¯:")
    print(f"   åƒç´ æ•°: {pixels:,} ({pixels/1_000_000:.2f}M)")
    print(f"   ä¼°ç®—æ˜¾å­˜: ~{estimated_gb:.1f}GB")

    # æ£€æµ‹å®é™…å¯ç”¨æ˜¾å­˜
    if torch.cuda.is_available():
        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"   GPU æ€»æ˜¾å­˜: {gpu_memory_gb:.1f}GB")

        if estimated_gb > gpu_memory_gb * 0.8:  # ä½¿ç”¨è¶…è¿‡ 80% æ˜¾å­˜
            print(f"\n   âŒ é”™è¯¯: ä¼°ç®—æ˜¾å­˜ ({estimated_gb:.1f}GB) è¶…è¿‡ GPU å®¹é‡ ({gpu_memory_gb:.1f}GB)")
            print(f"   å¿…é¡»é™ä½åˆ†è¾¨ç‡ï¼")
            print(f"\n   æ¨èé…ç½® (GPU {gpu_memory_gb:.0f}GB):")

            short_side = min(width, height)
            if gpu_memory_gb < 6:
                print(f"   --short-side 270  (ä¼°ç®— ~0.8GB)")
                print(f"   --short-side 360  (ä¼°ç®— ~1.4GB)")
            elif gpu_memory_gb < 8:
                print(f"   --short-side 360  (ä¼°ç®— ~1.4GB)")
                print(f"   --short-side 480  (ä¼°ç®— ~2.5GB)")
            elif gpu_memory_gb < 12:
                print(f"   --short-side 480  (ä¼°ç®— ~2.5GB)")
                print(f"   --short-side 540  (ä¼°ç®— ~3.1GB)")
            else:
                print(f"   --short-side 540  (ä¼°ç®— ~3.1GB)")
                print(f"   --short-side 720  (ä¼°ç®— ~5.5GB)")

            print(f"\n   ç¤ºä¾‹å‘½ä»¤:")
            print(f"   python main.py -v video.mp4 -c model.pth --regions '...' --short-side 360")
            print()
            import sys
            sys.exit(1)
        elif estimated_gb > gpu_memory_gb * 0.6:  # ä½¿ç”¨è¶…è¿‡ 60% æ˜¾å­˜
            print(f"   âš ï¸  è­¦å‘Š: æ˜¾å­˜ä½¿ç”¨ç‡å¯èƒ½è¾ƒé«˜ï¼Œå»ºè®®é™ä½åˆ†è¾¨ç‡")

    if estimated_gb > 8:
        print(f"   ğŸ’¡ å»ºè®®:")
        print(f"   æ–¹å¼1: --scale 0.5 (é™ä½åˆ° {width//2}x{height//2})")
        # æ¨èçŸ­è¾¹å°ºå¯¸
        short_side = min(width, height)
        if short_side > 540:
            print(f"   æ–¹å¼2: --short-side 540 (æ¨èç”¨äºæ˜¾å­˜ 4-6GB)")
        if short_side > 720:
            print(f"   æ–¹å¼3: --short-side 720 (æ¨èç”¨äºæ˜¾å­˜ 6-8GB)")


# sample reference frames from the whole video
def get_ref_index(neighbor_ids, length):
    ref_index = []
    for i in range(0, length, ref_length):
        if not i in neighbor_ids:
            ref_index.append(i)
    return ref_index


# read frame-wise masks
def read_mask(mpath, video_length=None):
    """
    è¯»å–maskå›¾ç‰‡

    å‚æ•°:
        mpath: maskç›®å½•è·¯å¾„
        video_length: è§†é¢‘æ€»å¸§æ•°ï¼Œå¦‚æœæä¾›ä¸”ç›®å½•åªæœ‰ä¸€å¼ å›¾ç‰‡ï¼Œåˆ™å¤ç”¨è¯¥å›¾ç‰‡

    è¿”å›:
        masks: maskå›¾ç‰‡åˆ—è¡¨
    """
    masks = []
    mnames = [f for f in os.listdir(mpath) if f.endswith('.png') or f.endswith('.jpg')]
    mnames.sort()

    # å¦‚æœåªæœ‰ä¸€å¼ maskå›¾ç‰‡ï¼Œä¸”æŒ‡å®šäº†è§†é¢‘é•¿åº¦ï¼Œåˆ™å¤ç”¨è¿™å¼ å›¾ç‰‡
    if len(mnames) == 1 and video_length is not None:
        print(f"æ£€æµ‹åˆ°å•å¼ maskï¼Œå°†å¤ç”¨äºæ‰€æœ‰ {video_length} å¸§")
        m = Image.open(os.path.join(mpath, mnames[0]))
        m = m.resize((w, h), Image.NEAREST)
        m = np.array(m.convert('L'))
        m = np.array(m > 0).astype(np.uint8)
        m = cv2.dilate(m, cv2.getStructuringElement(
            cv2.MORPH_CROSS, (3, 3)), iterations=4)
        mask_img = Image.fromarray(m*255)
        # å¤ç”¨åŒä¸€å¼ å›¾ç‰‡
        masks = [mask_img] * video_length
    else:
        # é€å¸§è¯»å–
        for m in mnames:
            m = Image.open(os.path.join(mpath, m))
            m = m.resize((w, h), Image.NEAREST)
            m = np.array(m.convert('L'))
            m = np.array(m > 0).astype(np.uint8)
            m = cv2.dilate(m, cv2.getStructuringElement(
                cv2.MORPH_CROSS, (3, 3)), iterations=4)
            masks.append(Image.fromarray(m*255))

    return masks


#  read frames from video
def read_frame_from_videos(vname):
    frames = []
    vidcap = cv2.VideoCapture(vname)
    success, image = vidcap.read()
    count = 0
    while success:
        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        frames.append(image.resize((w,h)))
        success, image = vidcap.read()
        count += 1
    return frames


def generate_masks_from_regions(video_path, regions, num_frames=None):
    """
    æ ¹æ®åŒºåŸŸåæ ‡ç”Ÿæˆmaskå›¾ç‰‡ï¼Œä¿å­˜åˆ°outputç›®å½•

    å‚æ•°:
        video_path: è§†é¢‘è·¯å¾„
        regions: åŒºåŸŸåˆ—è¡¨ï¼Œæ ¼å¼ä¸º [[left, bottom, right, top], ...]
                 åæ ‡ä¸ºç›¸å¯¹æ¯”ä¾‹ï¼ŒèŒƒå›´ 0-1ï¼Œä»¥å·¦ä¸‹è§’ä¸ºåŸç‚¹
                 left: å·¦è¾¹è·ç¦» (0=æœ€å·¦, 1=æœ€å³)
                 bottom: åº•è¾¹è·ç¦» (0=æœ€åº•, 1=æœ€é¡¶)
                 right: å³è¾¹è·ç¦» (0=æœ€å·¦, 1=æœ€å³)
                 top: é¡¶è¾¹è·ç¦» (0=æœ€åº•, 1=æœ€é¡¶)
        num_frames: å¸§æ•°ï¼ˆå¦‚æœä¸ºNoneåˆ™ä»è§†é¢‘ä¸­è·å–ï¼‰

    è¿”å›:
        mask_dir: maskç›®å½•è·¯å¾„
    """
    import datetime

    # è·å–è§†é¢‘ä¿¡æ¯
    vidcap = cv2.VideoCapture(video_path)

    if num_frames is None:
        num_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))

    # è·å–åŸå§‹è§†é¢‘å°ºå¯¸
    orig_width = int(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH))
    orig_height = int(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    vidcap.release()

    print(f"è§†é¢‘åŸå§‹å°ºå¯¸: {orig_width}x{orig_height}")
    print(f"å¤„ç†å°ºå¯¸: {w}x{h}")
    print(f"è§†é¢‘æ€»å¸§æ•°: {num_frames}")
    print(f"åŒºåŸŸæ•°é‡: {len(regions)}")

    # åˆ›å»ºoutputç›®å½•ä¸‹çš„maskå­ç›®å½•
    output_base = "output"
    os.makedirs(output_base, exist_ok=True)

    # ä½¿ç”¨æ—¶é—´æˆ³åˆ›å»ºå”¯ä¸€çš„maskç›®å½•
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    video_name = os.path.splitext(os.path.basename(video_path))[0]
    mask_dir = os.path.join(output_base, f"masks_{video_name}_{timestamp}")
    os.makedirs(mask_dir, exist_ok=True)
    print(f"Maskç›®å½•: {mask_dir}")

    # åˆ›å»ºé»‘è‰²èƒŒæ™¯ (ä½¿ç”¨å¤„ç†å°ºå¯¸)
    mask = np.zeros((h, w), dtype=np.uint8)

    # åœ¨æŒ‡å®šåŒºåŸŸç»˜åˆ¶ç™½è‰²
    for region in regions:
        left, bottom, right, top = region

        # éªŒè¯åæ ‡èŒƒå›´
        if not (0 <= left < right <= 1 and 0 <= bottom < top <= 1):
            raise ValueError(f"æ— æ•ˆçš„åŒºåŸŸåæ ‡: {region}ï¼Œåæ ‡å¿…é¡»åœ¨[0,1]èŒƒå›´å†…ä¸”left<right, bottom<top")

        # è½¬æ¢ç›¸å¯¹åæ ‡åˆ°å®é™…åƒç´ åæ ‡ï¼ˆåŸºäºå¤„ç†å°ºå¯¸ï¼‰
        # æ³¨æ„ï¼šä»¥å·¦ä¸‹è§’ä¸ºåŸç‚¹ï¼Œéœ€è¦è½¬æ¢Yåæ ‡
        x1 = int(left * w)
        x2 = int(right * w)

        # Yåæ ‡è½¬æ¢ï¼šå·¦ä¸‹è§’åŸç‚¹ -> å·¦ä¸Šè§’åŸç‚¹ï¼ˆå›¾åƒåæ ‡ç³»ï¼‰
        # bottom=0 è¡¨ç¤ºæœ€åº•éƒ¨ï¼Œåœ¨å›¾åƒåæ ‡ç³»ä¸­æ˜¯ h
        # top=1 è¡¨ç¤ºæœ€é¡¶éƒ¨ï¼Œåœ¨å›¾åƒåæ ‡ç³»ä¸­æ˜¯ 0
        y1 = int((1 - top) * h)      # é¡¶éƒ¨åœ¨å›¾åƒåæ ‡ç³»ä¸­çš„ä½ç½®
        y2 = int((1 - bottom) * h)   # åº•éƒ¨åœ¨å›¾åƒåæ ‡ç³»ä¸­çš„ä½ç½®

        # ç¡®ä¿åæ ‡åœ¨æœ‰æ•ˆèŒƒå›´å†…
        x1 = max(0, min(x1, w - 1))
        x2 = max(0, min(x2, w))
        y1 = max(0, min(y1, h - 1))
        y2 = max(0, min(y2, h))

        # ç»˜åˆ¶ç™½è‰²åŒºåŸŸ
        mask[y1:y2, x1:x2] = 255

        print(f"  åŒºåŸŸ [left={left}, bottom={bottom}, right={right}, top={top}]")
        print(f"    -> åƒç´ åæ ‡: x=[{x1},{x2}], y=[{y1},{y2}] (å›¾åƒåæ ‡ç³»)")

    # åº”ç”¨è†¨èƒ€æ“ä½œï¼ˆä¸read_maskä¿æŒä¸€è‡´ï¼‰
    mask = cv2.dilate(mask, cv2.getStructuringElement(
        cv2.MORPH_CROSS, (3, 3)), iterations=4)

    # åªä¿å­˜ä¸€å¼ maskå›¾ç‰‡
    mask_path = os.path.join(mask_dir, 'mask.png')
    Image.fromarray(mask).save(mask_path)

    print(f"âœ“ æˆåŠŸç”Ÿæˆå•å¼ mask (å°†å¤ç”¨äºæ‰€æœ‰ {num_frames} å¸§)")
    print(f"âœ“ èŠ‚çœç©ºé—´: ~{(num_frames - 1) * mask.nbytes / 1024 / 1024:.1f}MB")
    return mask_dir


def main_worker():
    # éªŒè¯å‚æ•°
    if args.mask is None and args.regions is None:
        raise ValueError("å¿…é¡»æŒ‡å®š --mask æˆ– --regions å‚æ•°ä¹‹ä¸€")
    if args.mask is not None and args.regions is not None:
        raise ValueError("--mask å’Œ --regions å‚æ•°ä¸èƒ½åŒæ—¶ä½¿ç”¨")

    # è®¾ç½®å¤„ç†åˆ†è¾¨ç‡
    print("\n" + "="*60)
    print("è§†é¢‘ä¿¡æ¯æ£€æµ‹")
    print("="*60)
    setup_resolution(args.video, args.resolution, args.scale, args.short_side)
    print("="*60 + "\n")

    # å…ˆç”Ÿæˆæˆ–å‡†å¤‡ maskï¼ˆåœ¨åŠ è½½æ¨¡å‹ä¹‹å‰ï¼‰
    temp_mask_dir = None  # ç”¨äºè·Ÿè¸ªä¸´æ—¶ç›®å½•
    if args.regions is not None:
        # ä»åŒºåŸŸåæ ‡ç”Ÿæˆmask
        print("="*60)
        print("ç”Ÿæˆ Mask")
        print("="*60)
        regions = json.loads(args.regions)
        print(f"åŒºåŸŸåæ ‡: {regions}")

        # è·å–è§†é¢‘å¸§æ•°
        vidcap = cv2.VideoCapture(args.video)
        video_length = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))
        vidcap.release()

        temp_mask_dir = generate_masks_from_regions(args.video, regions, video_length)
        print("="*60 + "\n")
    else:
        # éªŒè¯maskç›®å½•å­˜åœ¨
        print("="*60)
        print("éªŒè¯ Mask ç›®å½•")
        print("="*60)
        if not os.path.exists(args.mask):
            raise ValueError(f"Maskç›®å½•ä¸å­˜åœ¨: {args.mask}")
        mask_files = [f for f in os.listdir(args.mask) if f.endswith('.png') or f.endswith('.jpg')]
        if not mask_files:
            raise ValueError(f"Maskç›®å½•ä¸­æ²¡æœ‰å›¾ç‰‡æ–‡ä»¶: {args.mask}")
        print(f"âœ“ Maskç›®å½•: {args.mask}")
        print(f"âœ“ æ‰¾åˆ° {len(mask_files)} ä¸ªmaskæ–‡ä»¶")
        print("="*60 + "\n")

    # åŠ è½½æ¨¡å‹
    print("="*60)
    print("åŠ è½½æ¨¡å‹")
    print("="*60)
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"âœ“ ä½¿ç”¨è®¾å¤‡: {device} (GPU: {torch.cuda.get_device_name(0)})")
    else:
        device = torch.device("cpu")
        print(f"âœ“ ä½¿ç”¨è®¾å¤‡: CPU")

    net = importlib.import_module('src.model.' + args.model)
    model = net.InpaintGenerator().to(device)
    model_path = args.ckpt
    data = torch.load(args.ckpt, map_location=device, weights_only=False)
    model.load_state_dict(data['netG'])
    print(f'âœ“ æ¨¡å‹åŠ è½½å®Œæˆ: {args.ckpt}')
    model.eval()
    print("="*60 + "\n")

    # åŠ è½½è§†é¢‘å¸§
    print("="*60)
    print("åŠ è½½è§†é¢‘å¸§")
    print("="*60)
    frames = read_frame_from_videos(args.video)
    video_length = len(frames)
    print(f"âœ“ åŠ è½½ {video_length} å¸§")
    feats = _to_tensors(frames).unsqueeze(0)*2-1
    frames = [np.array(f).astype(np.uint8) for f in frames]
    print("="*60 + "\n")

    # åŠ è½½mask
    print("="*60)
    print("åŠ è½½ Mask")
    print("="*60)
    if temp_mask_dir is not None:
        masks = read_mask(temp_mask_dir, video_length)
    else:
        masks = read_mask(args.mask, video_length)
    print(f"âœ“ åŠ è½½ {len(masks)} ä¸ªmask")
    print("="*60 + "\n")

    binary_masks = [np.expand_dims((np.array(m) != 0).astype(np.uint8), 2) for m in masks]
    masks = _to_tensors(masks).unsqueeze(0)
    feats, masks = feats.to(device), masks.to(device)
    comp_frames = [None]*video_length

    # ç¼–ç ç‰¹å¾
    print("="*60)
    print("ç¼–ç è§†é¢‘ç‰¹å¾")
    print("="*60)
    with torch.no_grad():
        feats = model.encoder((feats*(1-masks).float()).view(video_length, 3, h, w))
        _, c, feat_h, feat_w = feats.size()
        feats = feats.view(1, video_length, c, feat_h, feat_w)
    print(f'âœ“ ç‰¹å¾ç¼–ç å®Œæˆ: {video_length} å¸§')
    print("="*60 + "\n")

    # ä¿®å¤è§†é¢‘
    print("="*60)
    print("å¼€å§‹ä¿®å¤è§†é¢‘")
    print("="*60)
    total_steps = (video_length + neighbor_stride - 1) // neighbor_stride
    for step, f in enumerate(range(0, video_length, neighbor_stride), 1):
        neighbor_ids = [i for i in range(max(0, f-neighbor_stride), min(video_length, f+neighbor_stride+1))]
        ref_ids = get_ref_index(neighbor_ids, video_length)

        print(f"å¤„ç†è¿›åº¦: {step}/{total_steps} (å¸§ {f}-{min(f+neighbor_stride, video_length)}/{video_length})", end='\r')

        with torch.no_grad():
            pred_feat = model.infer(
                feats[0, neighbor_ids+ref_ids, :, :, :], masks[0, neighbor_ids+ref_ids, :, :, :])
            pred_img = torch.tanh(model.decoder(
                pred_feat[:len(neighbor_ids), :, :, :])).detach()
            pred_img = (pred_img + 1) / 2
            pred_img = pred_img.cpu().permute(0, 2, 3, 1).numpy()*255
            for i in range(len(neighbor_ids)):
                idx = neighbor_ids[i]
                img = np.array(pred_img[i]).astype(
                    np.uint8)*binary_masks[idx] + frames[idx] * (1-binary_masks[idx])
                if comp_frames[idx] is None:
                    comp_frames[idx] = img
                else:
                    comp_frames[idx] = comp_frames[idx].astype(
                        np.float32)*0.5 + img.astype(np.float32)*0.5
    print()  # æ¢è¡Œ
    print(f'âœ“ è§†é¢‘ä¿®å¤å®Œæˆ')
    print("="*60 + "\n")
    # ç¡®å®šè¾“å‡ºè·¯å¾„
    if args.output:
        output_path = args.output
    else:
        # é»˜è®¤è¾“å‡ºåˆ°outputç›®å½•
        output_base = "output"
        os.makedirs(output_base, exist_ok=True)

        if args.mask:
            # ä½¿ç”¨maskç›®å½•åç§°
            mask_basename = os.path.basename(args.mask.rstrip('/'))
            output_path = os.path.join(output_base, f"{mask_basename}_result.mp4")
        else:
            # ä½¿ç”¨è§†é¢‘åç§°
            video_name = os.path.splitext(os.path.basename(args.video))[0]
            import datetime
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = os.path.join(output_base, f"{video_name}_inpainted_{timestamp}.mp4")

    # ä¿å­˜è¾“å‡ºè§†é¢‘
    print("="*60)
    print("ä¿å­˜è¾“å‡ºè§†é¢‘")
    print("="*60)
    print(f"è¾“å‡ºè·¯å¾„: {output_path}")

    writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*"mp4v"), default_fps, (w, h))
    for f in range(video_length):
        comp = np.array(comp_frames[f]).astype(
            np.uint8)*binary_masks[f] + frames[f] * (1-binary_masks[f])
        writer.write(cv2.cvtColor(np.array(comp).astype(np.uint8), cv2.COLOR_BGR2RGB))
        print(f"å†™å…¥å¸§: {f+1}/{video_length}", end='\r')
    writer.release()
    print()  # æ¢è¡Œ
    print(f'âœ“ è§†é¢‘ä¿å­˜å®Œæˆ: {output_path}')

    # ä¿ç•™maskç›®å½•ä¾›ç”¨æˆ·æŸ¥çœ‹
    if temp_mask_dir is not None:
        print(f'âœ“ Maskæ–‡ä»¶ä¿å­˜åœ¨: {temp_mask_dir}')
    print("="*60 + "\n")

    print("ğŸ‰ å…¨éƒ¨å®Œæˆï¼")



if __name__ == '__main__':
    main_worker()

# -*- coding: utf-8 -*-
import cv2
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
import math
import time
import importlib
import os
import argparse
import copy
import datetime
import random
import sys
import json
import subprocess

import torch
from torch.autograd import Variable

import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init
import torch.utils.model_zoo as model_zoo
from torchvision import models
import torch.multiprocessing as mp
from torchvision import transforms

# My libs
from src.core.utils import Stack, ToTorchFormatTensor


parser = argparse.ArgumentParser(description="STTN")
parser.add_argument("-v", "--video", type=str, required=True, help="è¾“å…¥è§†é¢‘è·¯å¾„")
parser.add_argument("-m", "--mask",   type=str, help="maskå›¾ç‰‡ç›®å½•è·¯å¾„ï¼ˆä¸--regionsäºŒé€‰ä¸€ï¼‰")
parser.add_argument("-c", "--ckpt",   type=str, required=True, help="æ¨¡å‹checkpointè·¯å¾„")
parser.add_argument("--model",   type=str, default='sttn', help="æ¨¡å‹åç§°")
parser.add_argument("--regions", type=str, help="éœ€è¦æ“¦é™¤çš„åŒºåŸŸåæ ‡ï¼Œæ ¼å¼ï¼š[[left,bottom,right,top],...]ï¼Œåæ ‡ä¸ºç›¸å¯¹æ¯”ä¾‹(0-1)ï¼ŒåŸç‚¹åœ¨å·¦ä¸‹è§’")
parser.add_argument("--output", type=str, help="è¾“å‡ºè§†é¢‘è·¯å¾„ï¼ˆé»˜è®¤ï¼š{mask}_result.mp4ï¼‰")
parser.add_argument("--resolution", type=str, help="å¤„ç†åˆ†è¾¨ç‡ï¼Œæ ¼å¼ï¼šWxH (å¦‚ 1280x720)ï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹")
parser.add_argument("--scale", type=float, default=1.0, help="åˆ†è¾¨ç‡ç¼©æ”¾æ¯”ä¾‹ (0-1]ï¼Œé»˜è®¤1.0ä¿æŒåŸåˆ†è¾¨ç‡")
parser.add_argument("--short-side", type=int, choices=[270, 360, 480, 540, 720, 1080],
                    help="ç­‰æ¯”ç¼©æ”¾åˆ°æŒ‡å®šçŸ­è¾¹å°ºå¯¸ï¼Œä¿æŒå®½é«˜æ¯” (270/360/480/540/720/1080)")
parser.add_argument("--crop", action="store_true",
                    help="è£å‰ªæ¨¡å¼ï¼šä»…å¤„ç†åŒºåŸŸå‘¨å›´éƒ¨åˆ†ï¼Œå¤§å¹…é™ä½æ˜¾å­˜å ç”¨ï¼ˆéœ€é…åˆ--regionsä½¿ç”¨ï¼‰")
parser.add_argument("--crop-padding", type=int, default=32,
                    help="è£å‰ªæ—¶çš„è¾¹ç•Œpaddingï¼ˆåƒç´ ï¼‰ï¼Œé¿å…è¾¹ç•Œartifactsï¼Œé»˜è®¤32")
args = parser.parse_args()


# é»˜è®¤å€¼ï¼Œå°†åœ¨ get_video_info ä¸­æ›´æ–°
w, h = 432, 240
ref_length = 10
neighbor_stride = 5
default_fps = 24

_to_tensors = transforms.Compose([
    Stack(),
    ToTorchFormatTensor()])


def get_video_info_ffprobe(video_path):
    """
    ä½¿ç”¨ ffprobe è·å–è§†é¢‘ä¿¡æ¯

    è¿”å›: (width, height, fps) æˆ– None
    """
    try:
        cmd = [
            'ffprobe',
            '-v', 'error',
            '-select_streams', 'v:0',
            '-show_entries', 'stream=width,height,r_frame_rate',
            '-of', 'json',
            video_path
        ]

        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)

        if result.returncode == 0:
            data = json.loads(result.stdout)
            if 'streams' in data and len(data['streams']) > 0:
                stream = data['streams'][0]
                width = stream.get('width')
                height = stream.get('height')

                # è§£æå¸§ç‡ (æ ¼å¼å¯èƒ½æ˜¯ "30/1" æˆ– "30000/1001")
                fps_str = stream.get('r_frame_rate', '24/1')
                if '/' in fps_str:
                    num, den = map(int, fps_str.split('/'))
                    fps = num / den if den != 0 else 24
                else:
                    fps = float(fps_str)

                return width, height, fps
    except (subprocess.TimeoutExpired, FileNotFoundError, json.JSONDecodeError) as e:
        print(f"âš ï¸  ffprobe è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}")

    return None


def get_video_info_opencv(video_path):
    """
    ä½¿ç”¨ OpenCV è·å–è§†é¢‘ä¿¡æ¯ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰

    è¿”å›: (width, height, fps)
    """
    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")

    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)

    cap.release()

    # å¦‚æœ fps æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤å€¼
    if fps <= 0 or fps > 120:
        fps = 24

    return width, height, fps


def setup_resolution(video_path, resolution_arg=None, scale=1.0, short_side=None):
    """
    è®¾ç½®å¤„ç†åˆ†è¾¨ç‡

    å‚æ•°:
        video_path: è§†é¢‘è·¯å¾„
        resolution_arg: ç”¨æˆ·æŒ‡å®šçš„åˆ†è¾¨ç‡å­—ç¬¦ä¸² (å¦‚ "1280x720")
        scale: ç¼©æ”¾æ¯”ä¾‹
        short_side: çŸ­è¾¹ç›®æ ‡å°ºå¯¸ï¼ˆç­‰æ¯”ç¼©æ”¾ï¼‰

    è¿”å›: (width, height, fps)
    """
    global w, h, default_fps

    # 1. å¦‚æœç”¨æˆ·æŒ‡å®šäº†åˆ†è¾¨ç‡ï¼Œç›´æ¥ä½¿ç”¨
    if resolution_arg:
        try:
            w, h = map(int, resolution_arg.lower().split('x'))
            print(f"âœ“ ä½¿ç”¨æŒ‡å®šåˆ†è¾¨ç‡: {w}x{h}")
        except ValueError:
            print(f"âš ï¸  æ— æ•ˆçš„åˆ†è¾¨ç‡æ ¼å¼: {resolution_arg}ï¼Œå°†è‡ªåŠ¨æ£€æµ‹")
            resolution_arg = None

    # 2. è·å–è§†é¢‘ä¿¡æ¯
    if not resolution_arg:
        # ä¼˜å…ˆä½¿ç”¨ ffprobe
        video_info = get_video_info_ffprobe(video_path)

        # å¦‚æœ ffprobe å¤±è´¥ï¼Œä½¿ç”¨ OpenCV
        if video_info is None:
            print("âš ï¸  ffprobe ä¸å¯ç”¨ï¼Œä½¿ç”¨ OpenCV è·å–è§†é¢‘ä¿¡æ¯")
            video_info = get_video_info_opencv(video_path)

        orig_w, orig_h, fps = video_info

        # åº”ç”¨çŸ­è¾¹ç¼©æ”¾ï¼ˆä¼˜å…ˆçº§é«˜äº scaleï¼‰
        if short_side:
            current_short = min(orig_w, orig_h)
            current_long = max(orig_w, orig_h)

            # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
            scale_ratio = short_side / current_short

            # åº”ç”¨ç¼©æ”¾
            if orig_w < orig_h:  # ç«–å±
                w = short_side
                h = int(current_long * scale_ratio)
            else:  # æ¨ªå±
                h = short_side
                w = int(current_long * scale_ratio)

            print(f"âœ“ åŸå§‹åˆ†è¾¨ç‡: {orig_w}x{orig_h}")
            print(f"âœ“ ç­‰æ¯”ç¼©æ”¾åˆ°çŸ­è¾¹ {short_side}: {w}x{h}")
        # åº”ç”¨ç¼©æ”¾æ¯”ä¾‹
        elif scale != 1.0:
            if scale <= 0 or scale > 1:
                print(f"âš ï¸  æ— æ•ˆçš„ç¼©æ”¾æ¯”ä¾‹ {scale}ï¼Œä½¿ç”¨ 1.0")
                scale = 1.0
            w = int(orig_w * scale)
            h = int(orig_h * scale)
            print(f"âœ“ åŸå§‹åˆ†è¾¨ç‡: {orig_w}x{orig_h}")
            print(f"âœ“ ç¼©æ”¾æ¯”ä¾‹: {scale}")
            print(f"âœ“ å¤„ç†åˆ†è¾¨ç‡: {w}x{h}")
        else:
            w = orig_w
            h = orig_h
            print(f"âœ“ è‡ªåŠ¨æ£€æµ‹åˆ†è¾¨ç‡: {w}x{h}")

        # æ›´æ–°å¸§ç‡
        default_fps = int(fps)
        print(f"âœ“ æ£€æµ‹åˆ°å¸§ç‡: {default_fps} fps")

    # 3. ç¡®ä¿åˆ†è¾¨ç‡æ˜¯å¶æ•°ï¼ˆè§†é¢‘ç¼–ç è¦æ±‚ï¼‰
    w = w if w % 2 == 0 else w + 1
    h = h if h % 2 == 0 else h + 1

    # 4. æ˜¾ç¤ºæ˜¾å­˜ä¼°ç®—
    estimate_memory(w, h)

    return w, h, default_fps


def estimate_memory(width, height):
    """
    ä¼°ç®—æ‰€éœ€æ˜¾å­˜
    """
    pixels = width * height
    # ç²—ç•¥ä¼°ç®—ï¼šæ¯ç™¾ä¸‡åƒç´ çº¦éœ€ 6GB æ˜¾å­˜
    estimated_gb = (pixels / 1_000_000) * 6

    print(f"ğŸ“Š åˆ†è¾¨ç‡ä¿¡æ¯:")
    print(f"   åƒç´ æ•°: {pixels:,} ({pixels/1_000_000:.2f}M)")
    print(f"   ä¼°ç®—æ˜¾å­˜: ~{estimated_gb:.1f}GB")

    # æ£€æµ‹å®é™…å¯ç”¨æ˜¾å­˜
    if torch.cuda.is_available():
        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"   GPU æ€»æ˜¾å­˜: {gpu_memory_gb:.1f}GB")

        if estimated_gb > gpu_memory_gb * 0.8:  # ä½¿ç”¨è¶…è¿‡ 80% æ˜¾å­˜
            print(f"\n   âŒ é”™è¯¯: ä¼°ç®—æ˜¾å­˜ ({estimated_gb:.1f}GB) è¶…è¿‡ GPU å®¹é‡ ({gpu_memory_gb:.1f}GB)")
            print(f"   å¿…é¡»é™ä½åˆ†è¾¨ç‡ï¼")
            print(f"\n   æ¨èé…ç½® (GPU {gpu_memory_gb:.0f}GB):")

            short_side = min(width, height)
            if gpu_memory_gb < 6:
                print(f"   --short-side 270  (ä¼°ç®— ~0.8GB)")
                print(f"   --short-side 360  (ä¼°ç®— ~1.4GB)")
            elif gpu_memory_gb < 8:
                print(f"   --short-side 360  (ä¼°ç®— ~1.4GB)")
                print(f"   --short-side 480  (ä¼°ç®— ~2.5GB)")
            elif gpu_memory_gb < 12:
                print(f"   --short-side 480  (ä¼°ç®— ~2.5GB)")
                print(f"   --short-side 540  (ä¼°ç®— ~3.1GB)")
            else:
                print(f"   --short-side 540  (ä¼°ç®— ~3.1GB)")
                print(f"   --short-side 720  (ä¼°ç®— ~5.5GB)")

            print(f"\n   ç¤ºä¾‹å‘½ä»¤:")
            print(f"   python main.py -v video.mp4 -c model.pth --regions '...' --short-side 360")
            print()
            import sys
            sys.exit(1)
        elif estimated_gb > gpu_memory_gb * 0.6:  # ä½¿ç”¨è¶…è¿‡ 60% æ˜¾å­˜
            print(f"   âš ï¸  è­¦å‘Š: æ˜¾å­˜ä½¿ç”¨ç‡å¯èƒ½è¾ƒé«˜ï¼Œå»ºè®®é™ä½åˆ†è¾¨ç‡")

    if estimated_gb > 8:
        print(f"   ğŸ’¡ å»ºè®®:")
        print(f"   æ–¹å¼1: --scale 0.5 (é™ä½åˆ° {width//2}x{height//2})")
        # æ¨èçŸ­è¾¹å°ºå¯¸
        short_side = min(width, height)
        if short_side > 540:
            print(f"   æ–¹å¼2: --short-side 540 (æ¨èç”¨äºæ˜¾å­˜ 4-6GB)")
        if short_side > 720:
            print(f"   æ–¹å¼3: --short-side 720 (æ¨èç”¨äºæ˜¾å­˜ 6-8GB)")


# sample reference frames from the whole video
def get_ref_index(neighbor_ids, length):
    ref_index = []
    for i in range(0, length, ref_length):
        if not i in neighbor_ids:
            ref_index.append(i)
    return ref_index


# read frame-wise masks
def read_mask(mpath, video_length=None):
    """
    è¯»å–maskå›¾ç‰‡

    å‚æ•°:
        mpath: maskç›®å½•è·¯å¾„
        video_length: è§†é¢‘æ€»å¸§æ•°ï¼Œå¦‚æœæä¾›ä¸”ç›®å½•åªæœ‰ä¸€å¼ å›¾ç‰‡ï¼Œåˆ™å¤ç”¨è¯¥å›¾ç‰‡

    è¿”å›:
        masks: maskå›¾ç‰‡åˆ—è¡¨
    """
    masks = []
    mnames = [f for f in os.listdir(mpath) if f.endswith('.png') or f.endswith('.jpg')]
    mnames.sort()

    # å¦‚æœåªæœ‰ä¸€å¼ maskå›¾ç‰‡ï¼Œä¸”æŒ‡å®šäº†è§†é¢‘é•¿åº¦ï¼Œåˆ™å¤ç”¨è¿™å¼ å›¾ç‰‡
    if len(mnames) == 1 and video_length is not None:
        print(f"æ£€æµ‹åˆ°å•å¼ maskï¼Œå°†å¤ç”¨äºæ‰€æœ‰ {video_length} å¸§")
        m = Image.open(os.path.join(mpath, mnames[0]))
        m = m.resize((w, h), Image.NEAREST)
        m = np.array(m.convert('L'))
        m = np.array(m > 0).astype(np.uint8)
        m = cv2.dilate(m, cv2.getStructuringElement(
            cv2.MORPH_CROSS, (3, 3)), iterations=4)
        mask_img = Image.fromarray(m*255)
        # å¤ç”¨åŒä¸€å¼ å›¾ç‰‡
        masks = [mask_img] * video_length
    else:
        # é€å¸§è¯»å–
        for m in mnames:
            m = Image.open(os.path.join(mpath, m))
            m = m.resize((w, h), Image.NEAREST)
            m = np.array(m.convert('L'))
            m = np.array(m > 0).astype(np.uint8)
            m = cv2.dilate(m, cv2.getStructuringElement(
                cv2.MORPH_CROSS, (3, 3)), iterations=4)
            masks.append(Image.fromarray(m*255))

    return masks


#  read frames from video
def read_frame_from_videos(vname):
    frames = []
    vidcap = cv2.VideoCapture(vname)
    success, image = vidcap.read()
    count = 0
    while success:
        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        frames.append(image.resize((w,h)))
        success, image = vidcap.read()
        count += 1
    return frames


def calculate_crop_region(regions, video_width, video_height, padding=32):
    """
    æ ¹æ®regionsè®¡ç®—è£å‰ªåŒºåŸŸï¼ˆåŠ paddingï¼‰

    å‚æ•°:
        regions: åŒºåŸŸåˆ—è¡¨ [[left,bottom,right,top],...]ï¼Œç›¸å¯¹åæ ‡(0-1)
        video_width, video_height: åŸè§†é¢‘å°ºå¯¸
        padding: è¾¹ç•Œpaddingï¼ˆåƒç´ ï¼‰

    è¿”å›:
        (x, y, crop_w, crop_h): è£å‰ªå‚æ•°ï¼ˆåƒç´ åæ ‡ï¼‰
        (min_left, min_bottom, max_right, max_top): åˆå¹¶åçš„åŒºåŸŸè¾¹ç•Œï¼ˆç›¸å¯¹åæ ‡ï¼‰
    """
    # åˆå¹¶æ‰€æœ‰åŒºåŸŸçš„è¾¹ç•Œ
    all_lefts = [r[0] for r in regions]
    all_bottoms = [r[1] for r in regions]
    all_rights = [r[2] for r in regions]
    all_tops = [r[3] for r in regions]

    min_left = min(all_lefts)
    min_bottom = min(all_bottoms)
    max_right = max(all_rights)
    max_top = max(all_tops)

    # è½¬æ¢ä¸ºåƒç´ åæ ‡
    x1_pixel = int(min_left * video_width)
    x2_pixel = int(max_right * video_width)

    # Yåæ ‡ï¼šå·¦ä¸‹è§’åŸç‚¹ -> å·¦ä¸Šè§’åŸç‚¹
    y1_pixel = int((1 - max_top) * video_height)
    y2_pixel = int((1 - min_bottom) * video_height)

    # æ·»åŠ padding
    x1_pixel = max(0, x1_pixel - padding)
    y1_pixel = max(0, y1_pixel - padding)
    x2_pixel = min(video_width, x2_pixel + padding)
    y2_pixel = min(video_height, y2_pixel + padding)

    # è®¡ç®—è£å‰ªåŒºåŸŸå°ºå¯¸
    crop_w = x2_pixel - x1_pixel
    crop_h = y2_pixel - y1_pixel

    # ç¡®ä¿å°ºå¯¸æ˜¯å¶æ•°
    crop_w = crop_w if crop_w % 2 == 0 else crop_w - 1
    crop_h = crop_h if crop_h % 2 == 0 else crop_h - 1

    return (x1_pixel, y1_pixel, crop_w, crop_h), (min_left, min_bottom, max_right, max_top)


def transform_regions_to_crop_space(regions, crop_region_bounds, video_width, video_height, crop_w, crop_h):
    """
    å°†å…¨å±€åæ ‡çš„regionsè½¬æ¢ä¸ºè£å‰ªç©ºé—´çš„ç›¸å¯¹åæ ‡

    å‚æ•°:
        regions: åŸå§‹åŒºåŸŸåˆ—è¡¨ï¼ˆå…¨å±€ç›¸å¯¹åæ ‡ï¼‰
        crop_region_bounds: (min_left, min_bottom, max_right, max_top) è£å‰ªåŒºåŸŸè¾¹ç•Œï¼ˆå…¨å±€ç›¸å¯¹åæ ‡ï¼‰
        video_width, video_height: åŸè§†é¢‘å°ºå¯¸
        crop_w, crop_h: è£å‰ªåçš„å°ºå¯¸

    è¿”å›:
        transformed_regions: è½¬æ¢åçš„åŒºåŸŸåˆ—è¡¨ï¼ˆè£å‰ªç©ºé—´ç›¸å¯¹åæ ‡ï¼‰
    """
    min_left, min_bottom, max_right, max_top = crop_region_bounds

    # è£å‰ªåŒºåŸŸçš„å°ºå¯¸ï¼ˆç›¸å¯¹åæ ‡ï¼‰
    crop_rel_width = max_right - min_left
    crop_rel_height = max_top - min_bottom

    transformed_regions = []
    for region in regions:
        left, bottom, right, top = region

        # è½¬æ¢åˆ°è£å‰ªç©ºé—´ï¼ˆç›¸å¯¹äºè£å‰ªåŒºåŸŸçš„å·¦ä¸‹è§’ï¼‰
        new_left = (left - min_left) / crop_rel_width
        new_right = (right - min_left) / crop_rel_width
        new_bottom = (bottom - min_bottom) / crop_rel_height
        new_top = (top - min_bottom) / crop_rel_height

        # ç¡®ä¿åœ¨[0,1]èŒƒå›´å†…
        new_left = max(0, min(1, new_left))
        new_right = max(0, min(1, new_right))
        new_bottom = max(0, min(1, new_bottom))
        new_top = max(0, min(1, new_top))

        transformed_regions.append([new_left, new_bottom, new_right, new_top])

    return transformed_regions


def crop_video_ffmpeg(input_video, output_video, x, y, width, height):
    """
    ä½¿ç”¨ffmpegè£å‰ªè§†é¢‘

    å‚æ•°:
        input_video: è¾“å…¥è§†é¢‘è·¯å¾„
        output_video: è¾“å‡ºè§†é¢‘è·¯å¾„
        x, y: è£å‰ªèµ·å§‹åæ ‡
        width, height: è£å‰ªå°ºå¯¸
    """
    cmd = [
        'ffmpeg', '-y', '-i', input_video,
        '-filter:v', f'crop={width}:{height}:{x}:{y}',
        '-c:a', 'copy',  # éŸ³é¢‘ç›´æ¥å¤åˆ¶
        output_video
    ]

    print(f"è£å‰ªå‘½ä»¤: {' '.join(cmd)}")
    result = subprocess.run(cmd, capture_output=True, text=True)

    if result.returncode != 0:
        raise RuntimeError(f"ffmpegè£å‰ªå¤±è´¥: {result.stderr}")

    return output_video


def merge_video_ffmpeg(original_video, inpainted_video, output_video, x, y):
    """
    ä½¿ç”¨ffmpegå°†ä¿®å¤åçš„è§†é¢‘åˆå¹¶å›åŸè§†é¢‘

    å‚æ•°:
        original_video: åŸå§‹è§†é¢‘è·¯å¾„
        inpainted_video: ä¿®å¤åçš„è£å‰ªè§†é¢‘è·¯å¾„
        output_video: è¾“å‡ºè§†é¢‘è·¯å¾„
        x, y: overlayä½ç½®
    """
    cmd = [
        'ffmpeg', '-y',
        '-i', original_video,
        '-i', inpainted_video,
        '-filter_complex', f'[0:v][1:v]overlay={x}:{y}',
        '-c:a', 'copy',  # ä¿ç•™åŸè§†é¢‘éŸ³é¢‘
        output_video
    ]

    print(f"åˆå¹¶å‘½ä»¤: {' '.join(cmd)}")
    result = subprocess.run(cmd, capture_output=True, text=True)

    if result.returncode != 0:
        raise RuntimeError(f"ffmpegåˆå¹¶å¤±è´¥: {result.stderr}")

    return output_video


def generate_masks_from_regions(video_path, regions, num_frames=None):
    """
    æ ¹æ®åŒºåŸŸåæ ‡ç”Ÿæˆmaskå›¾ç‰‡ï¼Œä¿å­˜åˆ°outputç›®å½•

    å‚æ•°:
        video_path: è§†é¢‘è·¯å¾„
        regions: åŒºåŸŸåˆ—è¡¨ï¼Œæ ¼å¼ä¸º [[left, bottom, right, top], ...]
                 åæ ‡ä¸ºç›¸å¯¹æ¯”ä¾‹ï¼ŒèŒƒå›´ 0-1ï¼Œä»¥å·¦ä¸‹è§’ä¸ºåŸç‚¹
                 left: å·¦è¾¹è·ç¦» (0=æœ€å·¦, 1=æœ€å³)
                 bottom: åº•è¾¹è·ç¦» (0=æœ€åº•, 1=æœ€é¡¶)
                 right: å³è¾¹è·ç¦» (0=æœ€å·¦, 1=æœ€å³)
                 top: é¡¶è¾¹è·ç¦» (0=æœ€åº•, 1=æœ€é¡¶)
        num_frames: å¸§æ•°ï¼ˆå¦‚æœä¸ºNoneåˆ™ä»è§†é¢‘ä¸­è·å–ï¼‰

    è¿”å›:
        mask_dir: maskç›®å½•è·¯å¾„
    """
    import datetime

    # è·å–è§†é¢‘ä¿¡æ¯
    vidcap = cv2.VideoCapture(video_path)

    if num_frames is None:
        num_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))

    # è·å–åŸå§‹è§†é¢‘å°ºå¯¸
    orig_width = int(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH))
    orig_height = int(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    vidcap.release()

    print(f"è§†é¢‘åŸå§‹å°ºå¯¸: {orig_width}x{orig_height}")
    print(f"å¤„ç†å°ºå¯¸: {w}x{h}")
    print(f"è§†é¢‘æ€»å¸§æ•°: {num_frames}")
    print(f"åŒºåŸŸæ•°é‡: {len(regions)}")

    # åˆ›å»ºoutputç›®å½•ä¸‹çš„maskå­ç›®å½•
    output_base = "output"
    os.makedirs(output_base, exist_ok=True)

    # ä½¿ç”¨æ—¶é—´æˆ³åˆ›å»ºå”¯ä¸€çš„maskç›®å½•
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    video_name = os.path.splitext(os.path.basename(video_path))[0]
    mask_dir = os.path.join(output_base, f"masks_{video_name}_{timestamp}")
    os.makedirs(mask_dir, exist_ok=True)
    print(f"Maskç›®å½•: {mask_dir}")

    # åˆ›å»ºé»‘è‰²èƒŒæ™¯ (ä½¿ç”¨å¤„ç†å°ºå¯¸)
    mask = np.zeros((h, w), dtype=np.uint8)

    # åœ¨æŒ‡å®šåŒºåŸŸç»˜åˆ¶ç™½è‰²
    for region in regions:
        left, bottom, right, top = region

        # éªŒè¯åæ ‡èŒƒå›´
        if not (0 <= left < right <= 1 and 0 <= bottom < top <= 1):
            raise ValueError(f"æ— æ•ˆçš„åŒºåŸŸåæ ‡: {region}ï¼Œåæ ‡å¿…é¡»åœ¨[0,1]èŒƒå›´å†…ä¸”left<right, bottom<top")

        # è½¬æ¢ç›¸å¯¹åæ ‡åˆ°å®é™…åƒç´ åæ ‡ï¼ˆåŸºäºå¤„ç†å°ºå¯¸ï¼‰
        # æ³¨æ„ï¼šä»¥å·¦ä¸‹è§’ä¸ºåŸç‚¹ï¼Œéœ€è¦è½¬æ¢Yåæ ‡
        x1 = int(left * w)
        x2 = int(right * w)

        # Yåæ ‡è½¬æ¢ï¼šå·¦ä¸‹è§’åŸç‚¹ -> å·¦ä¸Šè§’åŸç‚¹ï¼ˆå›¾åƒåæ ‡ç³»ï¼‰
        # bottom=0 è¡¨ç¤ºæœ€åº•éƒ¨ï¼Œåœ¨å›¾åƒåæ ‡ç³»ä¸­æ˜¯ h
        # top=1 è¡¨ç¤ºæœ€é¡¶éƒ¨ï¼Œåœ¨å›¾åƒåæ ‡ç³»ä¸­æ˜¯ 0
        y1 = int((1 - top) * h)      # é¡¶éƒ¨åœ¨å›¾åƒåæ ‡ç³»ä¸­çš„ä½ç½®
        y2 = int((1 - bottom) * h)   # åº•éƒ¨åœ¨å›¾åƒåæ ‡ç³»ä¸­çš„ä½ç½®

        # ç¡®ä¿åæ ‡åœ¨æœ‰æ•ˆèŒƒå›´å†…
        x1 = max(0, min(x1, w - 1))
        x2 = max(0, min(x2, w))
        y1 = max(0, min(y1, h - 1))
        y2 = max(0, min(y2, h))

        # ç»˜åˆ¶ç™½è‰²åŒºåŸŸ
        mask[y1:y2, x1:x2] = 255

        print(f"  åŒºåŸŸ [left={left}, bottom={bottom}, right={right}, top={top}]")
        print(f"    -> åƒç´ åæ ‡: x=[{x1},{x2}], y=[{y1},{y2}] (å›¾åƒåæ ‡ç³»)")

    # åº”ç”¨è†¨èƒ€æ“ä½œï¼ˆä¸read_maskä¿æŒä¸€è‡´ï¼‰
    mask = cv2.dilate(mask, cv2.getStructuringElement(
        cv2.MORPH_CROSS, (3, 3)), iterations=4)

    # åªä¿å­˜ä¸€å¼ maskå›¾ç‰‡
    mask_path = os.path.join(mask_dir, 'mask.png')
    Image.fromarray(mask).save(mask_path)

    print(f"âœ“ æˆåŠŸç”Ÿæˆå•å¼ mask (å°†å¤ç”¨äºæ‰€æœ‰ {num_frames} å¸§)")
    print(f"âœ“ èŠ‚çœç©ºé—´: ~{(num_frames - 1) * mask.nbytes / 1024 / 1024:.1f}MB")
    return mask_dir


def main_worker():
    # éªŒè¯å‚æ•°
    if args.mask is None and args.regions is None:
        raise ValueError("å¿…é¡»æŒ‡å®š --mask æˆ– --regions å‚æ•°ä¹‹ä¸€")
    if args.mask is not None and args.regions is not None:
        raise ValueError("--mask å’Œ --regions å‚æ•°ä¸èƒ½åŒæ—¶ä½¿ç”¨")
    if args.crop and args.regions is None:
        raise ValueError("--crop æ¨¡å¼å¿…é¡»é…åˆ --regions ä½¿ç”¨")

    # è£å‰ªæ¨¡å¼å˜é‡
    crop_mode = args.crop and args.regions is not None
    cropped_video = None
    crop_x, crop_y = 0, 0
    original_video = args.video

    # è£å‰ªæ¨¡å¼å¤„ç†
    if crop_mode:
        print("\n" + "="*60)
        print("ğŸ¯ è£å‰ªæ¨¡å¼ï¼šä»…å¤„ç†æ“¦é™¤åŒºåŸŸå‘¨å›´éƒ¨åˆ†")
        print("="*60)

        regions = json.loads(args.regions)

        # è·å–åŸè§†é¢‘ä¿¡æ¯
        video_info = get_video_info_ffprobe(original_video)
        if video_info is None:
            video_info = get_video_info_opencv(original_video)
        orig_w, orig_h, orig_fps = video_info

        print(f"åŸè§†é¢‘å°ºå¯¸: {orig_w}x{orig_h}")
        print(f"æ“¦é™¤åŒºåŸŸ: {regions}")
        print(f"è¾¹ç•Œpadding: {args.crop_padding}px")

        # è®¡ç®—è£å‰ªåŒºåŸŸ
        (crop_x, crop_y, crop_w, crop_h), crop_bounds = calculate_crop_region(
            regions, orig_w, orig_h, padding=args.crop_padding
        )

        print(f"\nè£å‰ªå‚æ•°:")
        print(f"  ä½ç½®: x={crop_x}, y={crop_y}")
        print(f"  å°ºå¯¸: {crop_w}x{crop_h}")
        print(f"  æ˜¾å­˜é™ä½: ~{(orig_w*orig_h)/(crop_w*crop_h):.1f}x")

        # è½¬æ¢regionsåˆ°è£å‰ªç©ºé—´
        transformed_regions = transform_regions_to_crop_space(
            regions, crop_bounds, orig_w, orig_h, crop_w, crop_h
        )
        print(f"  è½¬æ¢ååŒºåŸŸ: {transformed_regions}")

        # è£å‰ªè§†é¢‘
        print("\nè£å‰ªè§†é¢‘...")
        output_base = "output"
        os.makedirs(output_base, exist_ok=True)
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        cropped_video = os.path.join(output_base, f"cropped_{timestamp}.mp4")

        crop_video_ffmpeg(original_video, cropped_video, crop_x, crop_y, crop_w, crop_h)
        print(f"âœ“ è£å‰ªå®Œæˆ: {cropped_video}")

        # æ›´æ–°å¤„ç†ç›®æ ‡
        video_to_process = cropped_video
        regions_to_use = transformed_regions

        print("="*60 + "\n")
    else:
        video_to_process = original_video
        regions_to_use = json.loads(args.regions) if args.regions else None

    # è®¾ç½®å¤„ç†åˆ†è¾¨ç‡
    print("\n" + "="*60)
    print("è§†é¢‘ä¿¡æ¯æ£€æµ‹")
    print("="*60)
    setup_resolution(video_to_process, args.resolution, args.scale, args.short_side)
    print("="*60 + "\n")

    # å…ˆç”Ÿæˆæˆ–å‡†å¤‡ maskï¼ˆåœ¨åŠ è½½æ¨¡å‹ä¹‹å‰ï¼‰
    temp_mask_dir = None  # ç”¨äºè·Ÿè¸ªä¸´æ—¶ç›®å½•
    if args.regions is not None:
        # ä»åŒºåŸŸåæ ‡ç”Ÿæˆmask
        print("="*60)
        print("ç”Ÿæˆ Mask")
        print("="*60)
        print(f"åŒºåŸŸåæ ‡: {regions_to_use}")

        # è·å–è§†é¢‘å¸§æ•°
        vidcap = cv2.VideoCapture(video_to_process)
        video_length = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))
        vidcap.release()

        temp_mask_dir = generate_masks_from_regions(video_to_process, regions_to_use, video_length)
        print("="*60 + "\n")
    else:
        # éªŒè¯maskç›®å½•å­˜åœ¨
        print("="*60)
        print("éªŒè¯ Mask ç›®å½•")
        print("="*60)
        if not os.path.exists(args.mask):
            raise ValueError(f"Maskç›®å½•ä¸å­˜åœ¨: {args.mask}")
        mask_files = [f for f in os.listdir(args.mask) if f.endswith('.png') or f.endswith('.jpg')]
        if not mask_files:
            raise ValueError(f"Maskç›®å½•ä¸­æ²¡æœ‰å›¾ç‰‡æ–‡ä»¶: {args.mask}")
        print(f"âœ“ Maskç›®å½•: {args.mask}")
        print(f"âœ“ æ‰¾åˆ° {len(mask_files)} ä¸ªmaskæ–‡ä»¶")
        print("="*60 + "\n")

    # åŠ è½½æ¨¡å‹
    print("="*60)
    print("åŠ è½½æ¨¡å‹")
    print("="*60)
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"âœ“ ä½¿ç”¨è®¾å¤‡: {device} (GPU: {torch.cuda.get_device_name(0)})")
    else:
        device = torch.device("cpu")
        print(f"âœ“ ä½¿ç”¨è®¾å¤‡: CPU")

    net = importlib.import_module('src.model.' + args.model)
    model = net.InpaintGenerator().to(device)
    model_path = args.ckpt
    data = torch.load(args.ckpt, map_location=device, weights_only=False)
    model.load_state_dict(data['netG'])
    print(f'âœ“ æ¨¡å‹åŠ è½½å®Œæˆ: {args.ckpt}')
    model.eval()
    print("="*60 + "\n")

    # åŠ è½½è§†é¢‘å¸§
    print("="*60)
    print("åŠ è½½è§†é¢‘å¸§")
    print("="*60)
    frames = read_frame_from_videos(video_to_process)
    video_length = len(frames)
    print(f"âœ“ åŠ è½½ {video_length} å¸§")
    feats = _to_tensors(frames).unsqueeze(0)*2-1
    frames = [np.array(f).astype(np.uint8) for f in frames]
    print("="*60 + "\n")

    # åŠ è½½mask
    print("="*60)
    print("åŠ è½½ Mask")
    print("="*60)
    if temp_mask_dir is not None:
        masks = read_mask(temp_mask_dir, video_length)
    else:
        masks = read_mask(args.mask, video_length)
    print(f"âœ“ åŠ è½½ {len(masks)} ä¸ªmask")
    print("="*60 + "\n")

    binary_masks = [np.expand_dims((np.array(m) != 0).astype(np.uint8), 2) for m in masks]
    masks = _to_tensors(masks).unsqueeze(0)
    feats, masks = feats.to(device), masks.to(device)
    comp_frames = [None]*video_length

    # ç¼–ç ç‰¹å¾
    print("="*60)
    print("ç¼–ç è§†é¢‘ç‰¹å¾")
    print("="*60)
    with torch.no_grad():
        feats = model.encoder((feats*(1-masks).float()).view(video_length, 3, h, w))
        _, c, feat_h, feat_w = feats.size()
        feats = feats.view(1, video_length, c, feat_h, feat_w)
    print(f'âœ“ ç‰¹å¾ç¼–ç å®Œæˆ: {video_length} å¸§')
    print("="*60 + "\n")

    # ä¿®å¤è§†é¢‘
    print("="*60)
    print("å¼€å§‹ä¿®å¤è§†é¢‘")
    print("="*60)
    total_steps = (video_length + neighbor_stride - 1) // neighbor_stride
    for step, f in enumerate(range(0, video_length, neighbor_stride), 1):
        neighbor_ids = [i for i in range(max(0, f-neighbor_stride), min(video_length, f+neighbor_stride+1))]
        ref_ids = get_ref_index(neighbor_ids, video_length)

        print(f"å¤„ç†è¿›åº¦: {step}/{total_steps} (å¸§ {f}-{min(f+neighbor_stride, video_length)}/{video_length})", end='\r')

        with torch.no_grad():
            pred_feat = model.infer(
                feats[0, neighbor_ids+ref_ids, :, :, :], masks[0, neighbor_ids+ref_ids, :, :, :])
            pred_img = torch.tanh(model.decoder(
                pred_feat[:len(neighbor_ids), :, :, :])).detach()
            pred_img = (pred_img + 1) / 2
            pred_img = pred_img.cpu().permute(0, 2, 3, 1).numpy()*255
            for i in range(len(neighbor_ids)):
                idx = neighbor_ids[i]
                img = np.array(pred_img[i]).astype(
                    np.uint8)*binary_masks[idx] + frames[idx] * (1-binary_masks[idx])
                if comp_frames[idx] is None:
                    comp_frames[idx] = img
                else:
                    comp_frames[idx] = comp_frames[idx].astype(
                        np.float32)*0.5 + img.astype(np.float32)*0.5
    print()  # æ¢è¡Œ
    print(f'âœ“ è§†é¢‘ä¿®å¤å®Œæˆ')
    print("="*60 + "\n")

    # ç¡®å®šè¾“å‡ºè·¯å¾„
    if args.output:
        final_output_path = args.output
    else:
        # é»˜è®¤è¾“å‡ºåˆ°outputç›®å½•
        output_base = "output"
        os.makedirs(output_base, exist_ok=True)

        if args.mask:
            # ä½¿ç”¨maskç›®å½•åç§°
            mask_basename = os.path.basename(args.mask.rstrip('/'))
            final_output_path = os.path.join(output_base, f"{mask_basename}_result.mp4")
        else:
            # ä½¿ç”¨è§†é¢‘åç§°
            video_name = os.path.splitext(os.path.basename(args.video))[0]
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            final_output_path = os.path.join(output_base, f"{video_name}_inpainted_{timestamp}.mp4")

    # ä¿å­˜ä¿®å¤åçš„è§†é¢‘
    print("="*60)
    print("ä¿å­˜è¾“å‡ºè§†é¢‘")
    print("="*60)

    # è£å‰ªæ¨¡å¼ï¼šå…ˆä¿å­˜è£å‰ªåŒºåŸŸçš„ä¿®å¤è§†é¢‘
    if crop_mode:
        inpainted_crop_path = final_output_path.replace('.mp4', '_crop.mp4')
        print(f"ä¸­é—´æ–‡ä»¶: {inpainted_crop_path}")
    else:
        inpainted_crop_path = final_output_path
        print(f"è¾“å‡ºè·¯å¾„: {final_output_path}")

    writer = cv2.VideoWriter(inpainted_crop_path, cv2.VideoWriter_fourcc(*"mp4v"), default_fps, (w, h))
    for f in range(video_length):
        comp = np.array(comp_frames[f]).astype(
            np.uint8)*binary_masks[f] + frames[f] * (1-binary_masks[f])
        writer.write(cv2.cvtColor(np.array(comp).astype(np.uint8), cv2.COLOR_BGR2RGB))
        print(f"å†™å…¥å¸§: {f+1}/{video_length}", end='\r')
    writer.release()
    print()  # æ¢è¡Œ
    print(f'âœ“ è£å‰ªåŒºåŸŸä¿®å¤å®Œæˆ: {inpainted_crop_path}')
    print("="*60 + "\n")

    # è£å‰ªæ¨¡å¼ï¼šåˆå¹¶å›åŸè§†é¢‘
    if crop_mode:
        print("="*60)
        print("åˆå¹¶åˆ°åŸè§†é¢‘")
        print("="*60)
        print(f"åŸè§†é¢‘: {original_video}")
        print(f"ä¿®å¤åŒºåŸŸ: {inpainted_crop_path}")
        print(f"æœ€ç»ˆè¾“å‡º: {final_output_path}")
        print(f"Overlayä½ç½®: x={crop_x}, y={crop_y}")

        merge_video_ffmpeg(original_video, inpainted_crop_path, final_output_path, crop_x, crop_y)

        print(f'âœ“ åˆå¹¶å®Œæˆ: {final_output_path}')

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        print("\næ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        try:
            if cropped_video and os.path.exists(cropped_video):
                os.remove(cropped_video)
                print(f"âœ“ åˆ é™¤: {cropped_video}")
            if os.path.exists(inpainted_crop_path):
                os.remove(inpainted_crop_path)
                print(f"âœ“ åˆ é™¤: {inpainted_crop_path}")
        except Exception as e:
            print(f"âš ï¸  æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

        print("="*60 + "\n")

    # ä¿ç•™maskç›®å½•ä¾›ç”¨æˆ·æŸ¥çœ‹
    if temp_mask_dir is not None:
        print(f'âœ“ Maskæ–‡ä»¶ä¿å­˜åœ¨: {temp_mask_dir}')

    print("="*60)
    print("ğŸ‰ å…¨éƒ¨å®Œæˆï¼")
    print(f"ğŸ“¹ æœ€ç»ˆè¾“å‡º: {final_output_path}")
    print("="*60)



if __name__ == '__main__':
    main_worker()
